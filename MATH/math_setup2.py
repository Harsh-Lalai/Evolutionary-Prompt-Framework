import time
import openai
from openai import OpenAI
import csv
import os
import random
from random import sample
import json
import pickle
import multiprocessing
from multiprocessing import Pool
import datasets
from datasets import load_dataset
import boto3
from botocore.config import Config
import traceback
import pandas as pd
import math
import concurrent.futures
from evaluate import load
bertscore = load("bertscore")
rogue = load("rouge")

# Set the API key from the environment variable
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


# session = boto3.Session(profile_name="AuraIntern") # use this if you are using Ada. 

# # session = boto3.Session()


# config = Config(

#     read_timeout=120, # corresponds to inference time limit set for Bedrock 

#     connect_timeout=120,

#     retries={

#         "max_attempts": 5,

#     },

# )


# bedrock = session.client(

#     service_name='bedrock-runtime',

#     region_name="us-east-1",

#     endpoint_url="https://bedrock-runtime.us-east-1.amazonaws.com",

#     config=config

# )


def api_call_claude(model, prompt,temp = 0.5, max_tokens=2048):

    api_template = {

        "modelId": "anthropic.claude-3-sonnet-20240229-v1:0",

        "contentType": "application/json",

        "accept": "application/json",

        "body": ""

    }



    body = {

        "max_tokens":  max_tokens,

        "stop_sequences": ["\n\nHuman:"],

        "anthropic_version": "bedrock-2023-05-31",

        "temperature": 0.5,

        "messages": [{"role": "user", "content": prompt}]

    }



    api_template["body"] = json.dumps(body)



    success = False

    for i in range(5):

        try:

            response = bedrock.invoke_model(**api_template)

            success = True

            break

        except:

            

            traceback.print_exc()

            time.sleep(1)



    if success:

        response_body = json.loads(response.get('body').read())

        return response_body["content"][0]["text"].strip()

    else:

        print("***exception!!!!!")

        return ""

# Dictionary to store the ELO ratings of the prompts
elo_ratings_dict = {}
# Dictionary to store the history of the games in the leagues
league_dict = {}
#league number
current_league = 1
# Dictionary to store the top and bottom k prompts from each league
top_bottom_prompts_dict = {}
# Dictionary to store the human prompts which have been used in a league already
human_prompts_used_dict = {}

# Dictonary to store top k prompts across leagues  by using the final league
top_bottom_prompts_dict_across_league = {}

origin_league_prompts_dict = {}

top_bottom_prompts_dict_poc = {}


#Function for the API calls
def api_call_openai(model_name, prompt, temp, max_tokens):
    response = client.chat.completions.create(
        model= model_name,
        temperature= temp,
        messages=[{"role": "user", "content": prompt}],
        max_tokens= max_tokens
    )
    return response.choices[0].message.content.strip()


def api_call_openai_json(model_name, prompt, temp, max_tokens):
    response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
                temperature=temp,
                max_tokens=max_tokens
            )

    return json.loads(response.choices[0].message.content.strip())

# Function to get the last used Prompt ID from the CSV file
def get_last_prompt_id(file_name):
    """
    Parameters
    ----------
    file_name: str
        Name of the file where the prompts are stored
        
    Returns
    --------
    last_id: int
        The last Prompt ID used in the CSV file, or 0 if the file is empty or does not exist
    """
    last_id = 0
    if os.path.exists(file_name):
        with open(file_name, "r") as f:
            reader = csv.reader(f)
            for row in reader:
                if row[0] != "Prompt ID":  # Skip header
                    last_id = int(row[0])
    return last_id

# Function to get creative prompts from LLM
def get_prompts(no_of_prompts, league_no, temp, max_tokens, top_k_prompts=None, bottom_k_prompts=None, human=False, human_file=None):
    """
    Parameters
    ----------
    no_of_prompts: int
        Number of prompts required for the GSM8K game
    league_no: int
        The league number for which the prompts are to be generated
    top_k_prompts: list
        List of top k prompts from the previous league
    bottom_k_prompts: list
        List of bottom k prompts from the previous league
    human: bool
        True if the prompts are to be generated by humans, False if the prompts are to be generated by LLM
    human_file: str
        The name of the file where the human prompts are stored

    Returns
    --------
    prompts: list
        List of prompts generated by LLM for the GSM8K game to be given to the LLM
    """
    
    global human_prompts_used_dict

    if human and not human_file:
        print("Error in input! Please provide the human_file for the human prompts to be read.")
        return None
    
    if human:
        if (check_league(1)):
            with open('league_results.json', 'r') as json_file:
                data = json.load(json_file)
        
            # Collect all prompts with their final ELO scores
            prompts = set()
            for battle_no, battle_dict in data[str(league_no)].items():
                prompt_1 = battle_dict['prompt_1']
                prompt_2 = battle_dict['prompt_2']
                prompts.add(prompt_1['prompt'])
                prompts.add(prompt_2['prompt'])
            
            list(prompts)

            return prompts


        else: 
            prompts = set()
            with open(human_file, mode='r') as file:  
                csv_reader = csv.reader(file)
                for row in csv_reader:
                    if row[0] != "Prompt ID":
                        prompts.add(row[1])

            prompts = list(prompts)

            # Filter prompts that have already been used
            prompts = [prompt for prompt in prompts if prompt not in human_prompts_used_dict.values()]

            # Randomly sample the required number of prompts
            prompts = sample(prompts, no_of_prompts)
            return prompts

    else:
        # Read existing prompts and find the last used prompt ID
        existing_prompts = set()
        last_prompt_id = get_last_prompt_id(f"prompts_{league_no}.csv")

        if os.path.exists(f"prompts_{league_no}.csv"):
            with open(f"prompts_{league_no}.csv", mode='r') as file:  
                csv_reader = csv.reader(file)
                for row in csv_reader:
                    if row[0] != "Prompt ID":
                        existing_prompts.add(row[1])

        # # Generate the instruction for the LLM
        # if top_k_prompts is None and bottom_k_prompts is None:
        #     if len(existing_prompts)==0:
        #         instruction_for_prompts = """I am a Large Language Model about to solve a math problem. Help me make a prompt to excel in solving these problems.

        #         Make a creative, optimal prompt to help a large language model excel in solving the math problem effectively.

        #         Note that the prompt you are generating is independent of the specific type of math problem. You are only generating a prompt for the LLM to solve such problems effectively.

        #         Make sure that the response starts with 'Prompt: ' followed by the prompt text."""

        #     elif len(existing_prompts)!=0:
        #         instruction_for_prompts = f"""I am a Large Language Model about to solve a math problem. Help me make a prompt to excel in solving these problems.

        #         Make a creative, optimal prompt to help a large language model excel in solving the math problem effectively.

        #         Additionally, I already have a set of prompts that have been generated so far:

        #         Existing prompts:
        #         {existing_prompts}

        #         Do not combine, replicate, or produce anything similar to these existing prompts. Your goal is to create a completely new and unique prompt.

        #         Note that the prompt you are generating is independent of the specific type of math problem. You are only generating a prompt for the LLM to solve such problems effectively.

        #         Make sure that the response starts with 'Prompt: ' followed by the prompt text."""

        # else:
        #     top_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(top_k_prompts)])
        #     bottom_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(bottom_k_prompts)])
            
        #     if len(existing_prompts) == 0:
        #         instruction_for_prompts = f"""I am a Large Language Model about to solve a math problem. Help me make a prompt to excel in solving these problems.

        #     To assist you in generating better prompts, here are some examples of the best and worst prompts from previous games:

        #     Best prompts:
        #     {top_prompts_text}

        #     Worst prompts:
        #     {bottom_prompts_text}

        #     Use these examples to understand which strategies have worked well and which have not. However, your goal is not to combine or replicate the top prompts. Instead, use this insight to create a completely new and unique prompt that can outperform the previous best examples.
            
        #     Note that the prompt you are generating is independent of the specific type of math problem. You are only generating a prompt for the LLM to solve such problems effectively.

        #     Make sure that the response starts with 'Prompt: ' followed by the prompt text."""

        #     elif len(existing_prompts) != 0:
        #         instruction_for_prompts = f"""I am a Large Language Model about to solve a math problem. Help me make a prompt to excel in solving these problems.

        #     To assist you in generating better prompts, here are some examples of the best and worst prompts from previous games:

        #     Best prompts:
        #     {top_prompts_text}

        #     Worst prompts:
        #     {bottom_prompts_text}

        #     Existing prompts:
        #     {existing_prompts}

        #     Use the best and worst prompts to understand which strategies have worked well or poorly in the past. Your goal is not to combine, replicate, or produce anything similar to the existing or best prompts. Instead, use the insights from the best prompts to create a completely new and unique prompt that can outperform the previous best examples.

        #     Note that the prompt you are generating is independent of the specific type of math problem. You are only generating a prompt for the LLM to solve such problems effectively.

        #     Make sure that the response starts with 'Prompt: ' followed by the prompt text."""

        prompts = []
        prompt_set = existing_prompts.copy()

        while not check_prompts(no_of_prompts, f"prompts_{league_no}.csv"):

            times_runned = 0
            candidate_prompts_set = set()

            while(len(candidate_prompts_set)<4):

                if(times_runned>10):
                    print("Unable to generate prompts! Limit exceeded")
                    break

                json_response = api_call_openai_json("gpt-4o-mini-2024-07-18", instruction_for_prompts(top_k_prompts, bottom_k_prompts, existing_prompts, candidate_prompts_set), temp, max_tokens)
                prompt = extract_prompt(json_response)
                if prompt is None:
                    print(json_response)
                    times_runned += 1
                    print("Response did not contain the required keys, retrying...")
                    continue
                if prompt not in prompt_set:
                    candidate_prompts_set.add(prompt)
                else:
                    print("Prompt already exists in the prompt set, retrying...")
                    times_runned += 1
            
            best_prompt, reason = finding_best_prompt_from_candidates(existing_prompts, list(candidate_prompts_set), temp, max_tokens, top_k_prompts, bottom_k_prompts)

            if best_prompt not in prompt_set:
                prompt_set.add(best_prompt)
                existing_prompts.add(best_prompt)
                last_prompt_id += 1
                with open(f"prompts_{league_no}.csv", "a", newline="") as f:
                    writer = csv.writer(f)
                    if f.tell() == 0:
                        writer.writerow(["Prompt ID", "Prompt"])
                    writer.writerow([last_prompt_id, best_prompt])
            else:
                print("Best prompt already exists in the prompt set, retrying...")
                times_runned += 1
        
        print("Prompts generated successfully!")
        with open(f"prompts_{league_no}.csv", mode='r') as file:  
            csv_reader = csv.reader(file)
            for row in csv_reader:
                if row[0] != "Prompt ID":
                    prompts.append(row[1])
        print(prompts)
        # bert_score()
        # rogue_score()
        return prompts


def bert_score():
    """
    Calculate the BERT score for the prompts generated using the prompts csv files and store it in a json file.
    The index of the prompt is used as the key in the JSON file.
    """
    # Calculate the BERT score for the prompts_{current_league}.csv file with each other 
    df = pd.read_csv(f"prompts_{current_league}.csv")
    
    prompts = df["Prompt"].to_list()
    

    # Function to check if a key exists in JSON file
    def key_exists_in_json(filename, key):
        if os.path.exists(filename):
            with open(filename, "r") as f:
                data = json.load(f)
            return str(key) in data
        return False

    # Process Prompts
    for i in range(len(prompts) - 1):
        references = prompts[i + 1:]
        predictions = [prompts[i]] * len(references)
        
        # Check if the key for this prompt already exists in the JSON file
        if key_exists_in_json(f"bert_score_{current_league}.json", i):
            print(f"Skipping calculation for prompt {i}, already exists in the file.")
            continue
        
        # Compute the BERT score
        results = bertscore.compute(predictions=predictions, references=references, lang="en")

        # Check if the JSON file exists and update it
        if os.path.exists(f"bert_score_{current_league}.json"):
            with open(f"bert_score_{current_league}.json", "r") as f:
                data = json.load(f)
            data[str(i)] = results  # Store the results under the index `i`
        else:
            data = {str(i): results}

        # Write back to the JSON file
        with open(f"bert_score_{current_league}.json", "w") as f:
            json.dump(data, f)
    

    # Process cross-league prompts
    for i in range(1, current_league):
        if os.path.exists(f"prompts_{current_league-i}.csv"):
            df_prev = pd.read_csv(f"prompts_{current_league-i}.csv")
            prompts_prev = df_prev["Prompt"].to_list()

            # Process cross-league pro prompts
            for j in range(len(prompts)):
                if key_exists_in_json(f"bert_score_{current_league}_{current_league-i}.json", j):
                    print(f"Skipping calculation for cross-league prompt {j}, already exists in the file.")
                    continue
                
                references = prompts_prev
                predictions = [prompts[j]] * len(references)
                
                results = bertscore.compute(predictions=predictions, references=references, lang="en")
                
                # Check if the JSON file exists and update it
                if os.path.exists(f"bert_score_{current_league}_{current_league-i}.json"):
                    with open(f"bert_score_{current_league}_{current_league-i}.json", "r") as f:
                        data = json.load(f)
                    data[str(j)] = results  # Store the results under the index `j`
                else:
                    data = {str(j): results}

                # Write back to the JSON file
                with open(f"bert_score_{current_league}_{current_league-i}.json", "w") as f:
                    json.dump(data, f)
def rogue_score():
    """
    Calculate the ROUGE score for the prompts generated using the prompts csv files and store it in a json file.
    The index of the prompt pair is used as the key in the JSON file.
    """
    # Load prompts for the current league
    df = pd.read_csv(f"prompts_{current_league}.csv")
    prompts = df["Prompt"].to_list()
    
    # Function to check if a key exists in JSON file
    def key_exists_in_json(filename, key):
        if os.path.exists(filename):
            with open(filename, "r") as f:
                data = json.load(f)
            return str(key) in data
        return False

    # Process Prompts: Compare each prompt i with all subsequent prompts (i+1, i+2, ...)
    for i in range(len(prompts) - 1):
        for j in range(i + 1, len(prompts)):  # Compare prompt[i] with prompt[j] individually
            key = f"{i}_{j}"  # Unique key for each prompt pair

            # Check if the key for this prompt pair already exists in the JSON file
            if key_exists_in_json(f"rogue_score_{current_league}.json", key):
                print(f"Skipping calculation for prompt pair {i}-{j}, already exists in the file.")
                continue

            # Compute the ROUGE score between prompt[i] and prompt[j]
            results = rogue.compute(predictions=[prompts[i]], references=[prompts[j]], lang="en")

            # Check if the JSON file exists and update it
            if os.path.exists(f"rogue_score_{current_league}.json"):
                with open(f"rogue_score_{current_league}.json", "r") as f:
                    data = json.load(f)
                data[key] = results  # Store the results under the key `i_j`
            else:
                data = {key: results}

            # Write back to the JSON file
            with open(f"rogue_score_{current_league}.json", "w") as f:
                json.dump(data, f)

    # Process cross-league prompts
    for i in range(1, current_league):
        if os.path.exists(f"prompts_{current_league-i}.csv"):
            df_prev = pd.read_csv(f"prompts_{current_league-i}.csv")
            prompts_prev = df_prev["Prompt"].to_list()

            # Process cross-league prompts: Compare each prompt[j] from the current league with prompts[k] from previous leagues
            for j in range(len(prompts)):
                for k in range(len(prompts_prev)):  # Compare prompt[j] with each prompt from the previous league
                    key = f"{j}_{k}"  # Unique key for cross-league prompt pair

                    # Check if the key for this prompt pair already exists in the JSON file
                    if key_exists_in_json(f"rogue_score_{current_league}_{current_league-i}.json", key):
                        print(f"Skipping calculation for cross-league prompt pair {j}-{k}, already exists in the file.")
                        continue

                    # Compute the ROUGE score between prompt[j] and prompt_prev[k]
                    results = rogue.compute(predictions=[prompts[j]], references=[prompts_prev[k]], lang="en")

                    # Check if the JSON file exists and update it
                    if os.path.exists(f"rogue_score_{current_league}_{current_league-i}.json"):
                        with open(f"rogue_score_{current_league}_{current_league-i}.json", "r") as f:
                            data = json.load(f)
                        data[key] = results  # Store the results under the key `j_k`
                    else:
                        data = {key: results}

                    # Write back to the JSON file
                    with open(f"rogue_score_{current_league}_{current_league-i}.json", "w") as f:
                        json.dump(data, f)


def instruction_for_prompts(top_k_prompts, bottom_k_prompts, existing_prompts, candidates):
        
    existing_and_candidate_prompts = existing_prompts.union(candidates)
    existing_and_candidate_prompts_text = ""
    if len(existing_and_candidate_prompts) != 0:
        existing_and_candidate_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(existing_and_candidate_prompts)])
    if len(top_k_prompts) != 0:
        top_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(top_k_prompts)])
    if len(bottom_k_prompts) != 0:
        bottom_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(bottom_k_prompts)])

    if top_k_prompts is None and bottom_k_prompts is None:
        if len(existing_and_candidate_prompts) == 0:
            instructions = (
                "Help me create a prompt which I can provide to an AI model to solve a math problem.\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Your goal is to generate a completely new and unique prompt that offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all other prompts possible.\n"
                "-- The prompt you are generating is independent of the type of math problem. Focus on crafting a unique prompt for providing an AI model to solve a math problem.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all other prompts possbile]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )
        elif len(existing_and_candidate_prompts)!=0:

            instructions = (
                "Help me create a prompt which I can provide to an AI model to solve a math problem.\n\n"
                "Here is a list of existing prompts that have already been generated:\n"
                f"{existing_and_candidate_prompts_text}\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Do not combine, replicate, or produce anything similar to the above prompts.\n"
                "-- To ensure diversity in the prompts you generate, aim for lexical variety by minimizing the repetition of common words from above prompts.\n"
                "-- Your goal is to generate a completely new and unique prompt that is distinct from all of these and offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all the above prompts.\n"
                "-- The prompt you are generating is independent of the type of math problem. Focus on crafting a unique prompt for providing an AI model to solve a math problem.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all above prompts]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )
    if top_k_prompts is not None and bottom_k_prompts is not None:
        if len(existing_and_candidate_prompts) == 0:
            instructions = (
                "Help me create a prompt which I can provide to an AI model to solve a math problem.\n\n"
                "Here is a list of best and worst prompts:\n"
                f"Best prompts:\n{top_prompts_text}\n\nWorst prompts:\n{bottom_prompts_text}\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Do not combine, replicate, or produce anything similar to the above prompts.\n"
                "-- To ensure diversity in the prompts you generate, aim for lexical variety by minimizing the repetition of common words from above prompts.\n" 
                "-- Your goal is to generate a completely new and unique prompt that offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all the above prompts.\n"
                "-- The prompt you are generating is independent of the type of math problem. Focus on crafting a unique prompt for providing an AI model to solve a math problem.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all the above prompts]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "Ensure all fields are provided and have non-zero or non-empty values."
            )
        elif len(existing_and_candidate_prompts)!=0:
            instructions = (
                "Help me create a prompt which I can provide to an AI model to engage effectively in a debate as the Pro side debater.\n\n"
                "Here is a list of best and worst prompts:\n"
                f"Best prompts:\n{top_prompts_text}\n\nWorst prompts:\n{bottom_prompts_text}\n\n"
                "Here is a list of existing prompts that have already been generated:\n"
                f"{existing_and_candidate_prompts_text}\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Do not combine, replicate, or produce anything similar to the above prompts.\n"
                "-- To ensure diversity in the prompts you generate, aim for lexical variety by minimizing the repetition of common words from above prompts.\n"
                "-- Your goal is to generate a completely new and unique prompt that is distinct from all of these and offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all the above prompts.\n"
                "-- The prompt you are generating is independent of the type of math problem. Focus on crafting a unique prompt for providing an AI model to solve a math problem.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all the above prompts]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

    return instructions

# Function to extract prompt text from JSON response
def extract_prompt(json_response):
    """
    Extracts the prompt text from the JSON response if it meets all criteria.
    """
    # Define the required keys
    required_keys = ["Reason for diversity", "Is this diverse", "Reason for best prompt", "Is this the best prompt", "Prompt Text"]

    # Check if all required keys are present and their values are non-zero or non-empty
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the prompt text
        prompt_text = json_response["Prompt Text"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        return prompt_text
    else:
        # If any key is missing or has an empty/zero value, return None
        return None
    
# Function to check if the number of prompts required have been generated
def check_prompts(no_of_prompts, file_name):
    """
    Parameters
    ----------
    no_of_prompts: int
        Number of prompts required for the GSM8K game to be generated
    file_name: str
        The name of the file where the prompts are stored

    Returns
    --------
    bool
        True if the number of prompts stored in the prompts.csv are more than or equal to the number of prompts required, False otherwise
    """

    if not os.path.exists(file_name):
        return False

    with open(file_name, "r") as f:
        reader = csv.reader(f)
        data = list(reader)
        if len(data) >= no_of_prompts + 1:  # +1 for the header row
            return True
        else:
            return False

def extract_best_prompt(json_response):
    """
    Extracts the best prompt and reason from the JSON response if it meets all criteria.
    """
    # Define the required keys
    required_keys = ["Reason for the prompt being the best", "Prompt Text"]

    # Check if all required keys are present and their values are non-zero or non-empty
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the prompt text and reason
        reason = json_response["Reason for the prompt being the best"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        prompt_text = json_response["Prompt Text"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        return prompt_text, reason
    else:
        # If any key is missing or has an empty/zero value, return None
        return None, None
    
# Function to load GSM8K dataset from the Hugging face and return the questions list and corresponding answers list
def load_dataset_from_huggingface(no_of_questions):
    
    """
    Parameters
    ----------
    no_of_questions: int
        Number of questions required from the GSM8K dataset

    Returns
    --------
    questions: List
        List of questions from the GSM8K dataset
    answers: List
        List of answers from the GSM8K dataset
    """

    # Load a specific problem type and split
    dataset = load_dataset('deepmind/math_dataset', 'algebra__linear_2d', split='test')    
    questions = dataset["question"]
    answers = dataset["answer"]

    # Randomly chose no_of_questions indices from the dataset for the questions and answers
    indices = random.sample(range(len(questions)), no_of_questions)
    questions = [questions[i] for i in indices]
    answers = [answers[i] for i in indices]
    questions = [q.replace("b'", "").replace("\\n'", "").replace("'", "").strip() for q in questions]
    answers = [a.replace("b'", "").replace("\\n'", "").replace("'", "").strip() for a in answers]

    return questions, answers

# Finding the most diverse prompt from the 5 candidate prompts. The prompt has to be diverse from the existing prompts
def finding_best_prompt_from_candidates(existing_prompts, candidates, temp, max_tokens, top_k_prompts=None, bottom_k_prompts=None):

    if len(existing_prompts) != 0:
        existing_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(existing_prompts)])

    if top_k_prompts is None and bottom_k_prompts is None:
        if len(existing_prompts) != 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to solve a math problem.\n\n"
                "Here are the 3 candidate prompts from which you need to select the best one:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "The existing prompts are:\n"
                f"{existing_prompts_text}\n\n"
                "Consider the following criteria while evaluating each response:\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "any of the other candidate prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
                
            )

        elif len(existing_prompts) == 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to solve a math problem.\n\n"
                "Here are the 3 candidate prompts from which you need to select the best one:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "Consider the following criteria while evaluating each response:\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "any of the other candidate prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

    if top_k_prompts is not None and bottom_k_prompts is not None:

        top_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(top_k_prompts)])
        bottom_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(bottom_k_prompts)])

        if len(existing_prompts) != 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to solve a math problem.\n\n"
                "Here are the 3 candidate prompts from which you need to select the best one:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "The existing prompts are:\n"
                f"{existing_prompts_text}\n\n"
                "Additionally, here are some examples of the best and worst prompts:\n\n"
                "Best prompts:\n"
                f"{top_prompts_text}\n\n"
                "Worst prompts:\n"
                f"{bottom_prompts_text}\n\n"
                "Evaluate each response based on the following criteria:\n\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "all other candidate prompts as well as the best and worst prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

        elif len(existing_prompts) == 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to solve a math problem.\n\n"
                "Here are the 3 candidate prompts:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "Additionally, here are some examples of the best and worst prompts:\n\n"
                "Best prompts:\n"
                f"{top_prompts_text}\n\n"
                "Worst prompts:\n"
                f"{bottom_prompts_text}\n\n"
                "Evaluate each response based on the following criteria:\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "all other candidate prompts as well as the best and worst prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

    candidate_prompts = [candidates[0], candidates[1], candidates[2]]

    times_runned =0
    while(times_runned<10):
        json_response = api_call_openai_json("gpt-4o-mini-2024-07-18", instruction_to_select_best_prompt, temp, max_tokens)
        best_prompt, reason = extract_best_prompt(json_response)
        if best_prompt is None or reason is None:
            print(json_response)
            print("Response did not contain the required keys, retrying...")
            times_runned += 1
            continue
        
         # Make a dictionary which has key as the prompt and value as a dictionary which has the reason and candidate prompts
        best_prompt_dict = {best_prompt: {"reason": reason, "candidates": candidates}}

        # Store this dict in a json file
        # Check if the json file already has data then update the data

        if os.path.exists(f"selected_prompt_dict.json"):
            with open(f"selected_prompt_dict.json", "r") as f:
                data = json.load(f)
                data.update(best_prompt_dict)
                with open(f"selected_prompt_dict.json", "w") as f:
                    json.dump(data, f)
        else:
            with open(f"selected_prompt_dict.json", "w") as f:
                json.dump(best_prompt_dict, f)

        return best_prompt, reason
        
    
# Function to play the GSM8K game
def game(question, answer, player_prompt, temp, max_tokens):

    times_runned = 0
    final_answer = answer
    # print("Question: ", question)
    # print("Answer: ", answer)

    start_time = time.time()

    while True:
        if(times_runned>5):
            print("Failed to give answer in the correct format")
            return question, answer, final_answer, None, None, False

        try:
            response_text = None
            response_text = api_call_openai_json("gpt-4o-mini-2024-07-18", prompt_to_player(question, player_prompt), temp, max_tokens)
        except:
            print("Failed to get json response from the API, retrying...")
            print(response_text)
            times_runned += 1
            continue

        solution, final_answer_given = extract_answer(response_text)
        if solution is None or final_answer_given is None:
            print(response_text)
            print("Response did not contain the required keys, retrying...")
            times_runned += 1
            continue
        else:
            break

    end_time = time.time()
    print(f"Time taken to complete the game: {end_time - start_time} seconds")
    print("Game completed!")
    # Check if the final answer given by the player is correct or not
   # Ensure both final_answer_given and final_answer are stripped and converted to the same type
    if str(final_answer_given).strip() == str(final_answer).strip():
        print("Congratulations! You have given the correct answer.")
        print(response_text)
        answered_correctly = True
    else:
        print("Sorry! The answer given by you is incorrect.")
        print("Question: ", question)
        print("Solution given: ", solution)
        print("Final Answer: ", final_answer_given)
        print("Actual Answer: ", final_answer)
        answered_correctly = False

    results = [question, answer, final_answer, solution, final_answer_given, answered_correctly]
    return results

def extract_answer(json_response):
    # Check if the json response has the required keys and the values are non-empty
    keys = ["Solution", "Final_Answer"]
    if all(key in json_response for key in keys) and all(str(json_response[key]).strip() for key in keys):
        # Clean up the solution and final answer formats
        solution = str(json_response["Solution"]).strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        final_answer_given = str(json_response["Final_Answer"]).strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()

        # Check if the final answer is a single number (integer or float)
        try:
            final_answer_given = float(final_answer_given)  # Convert to float to handle integers and decimals
            if final_answer_given.is_integer():  # Convert to int if it's a whole number
                final_answer_given = int(final_answer_given)
            return solution, final_answer_given
        except ValueError:
            # If it cannot be converted to a float, it's not a valid number
            print("Final answer is not a valid number, retrying...")
            return None, None
    else:
        return None, None


def prompt_to_player(question,player_prompt):

    """
    Parameters:
    question (str): The question from the GSM8K dataset.
    player_prompt (str): The prompt to instruct the LLM.

    Returns:
    str: The combined prompt to be given to the LLM.
    """
    # Combine the player prompt, one-shot example, and the question using triple quotes
    combined_prompt = (player_prompt + 
                    "\n\nHere is the question you need to solve:\n\n"
                    "Question: " + question + "\n\n"
                    "Now, provide the solution and final answer in the following JSON format:\n\n"
                    "{\n"
                    '    "Solution": "[Provide your solution here]",\n'
                    '    "Final_Answer": "[Numerical value only]"\n'
                    "}\n\n"
                    )

    return combined_prompt


# finding the ELO rating of the prompt after battle
def elo_rating(prompt_1, prompt_2, question, answer, temp, max_tokens, game_over = False, result_1 = None, result_2 = None, custom_dict = None):
    
    global elo_ratings_dict
    if game_over == False:
        # Get the results of the game1
        result_1 = game(question, answer, prompt_1, temp, max_tokens)
        # Get the results of the game2
        result_2 = game(question, answer, prompt_2, temp, max_tokens)
    
    # Check if the prompt_1 has answered the question correctly or not
    answered_correctly_1 = result_1[5]
    # Check if the prompt_2 has answered the question correctly or not
    answered_correctly_2 = result_2[5]
    
    # If both the prompts have answered the question correctly, then the game is a draw
    if answered_correctly_1 == True and answered_correctly_2 == True:
        winner = "draw"
        loser = "draw"
    # If prompt_1 has answered the question correctly, then prompt_1 is the winner
    elif answered_correctly_1 == True:
        winner = prompt_1
        loser = prompt_2
    # If prompt_2 has answered the question correctly, then prompt_2 is the winner
    elif answered_correctly_2 == True:
        winner = prompt_2
        loser = prompt_1
    
    # If none of the prompts have answered the question correctly, then the game is a draw
    elif answered_correctly_1 == False and answered_correctly_2 == False:
        winner = "draw"
        loser = "draw"

    if custom_dict == None:
        #If it is a draw, then calculate the ELO ratings of the prompts and print the updated ELO ratings
        if winner == "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = elo_ratings_dict[prompt_1]
            loser_rating = elo_ratings_dict[prompt_2]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (0.5 - expected_winner)
            loser_rating = loser_rating + 32 * (0.5 - expected_loser)
            # Update the ELO ratings in the dictionary
            elo_ratings_dict[prompt_1] = winner_rating
            elo_ratings_dict[prompt_2] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{prompt_1}: {winner_rating}")
            print(f"{prompt_2}: {loser_rating}")

        # Update the ELO ratings of the prompts if it is not a draw
        elif winner != "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = elo_ratings_dict[winner]
            loser_rating = elo_ratings_dict[loser]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (1 - expected_winner)
            loser_rating = loser_rating + 32 * (0 - expected_loser)
            # Update the ELO ratings in the dictionary
            elo_ratings_dict[winner] = winner_rating
            elo_ratings_dict[loser] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{winner}: {winner_rating}")
            print(f"{loser}: {loser_rating}")

        return result_1, result_2, winner
    
    if custom_dict != None:
        #If it is a draw, then calculate the ELO ratings of the prompts and print the updated ELO ratings
        if winner == "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = custom_dict[prompt_1]
            loser_rating = custom_dict[prompt_2]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (0.5 - expected_winner)
            loser_rating = loser_rating + 32 * (0.5 - expected_loser)
            # Update the ELO ratings in the dictionary
            custom_dict[prompt_1] = winner_rating
            custom_dict[prompt_2] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{prompt_1}: {winner_rating}")
            print(f"{prompt_2}: {loser_rating}")

        # Update the ELO ratings of the prompts if it is not a draw
        elif winner != "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = custom_dict[winner]
            loser_rating = custom_dict[loser]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (1 - expected_winner)
            loser_rating = loser_rating + 32 * (0 - expected_loser)
            # Update the ELO ratings in the dictionary
            custom_dict[winner] = winner_rating
            custom_dict[loser] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{winner}: {winner_rating}")
            print(f"{loser}: {loser_rating}")

        return result_1, result_2, winner


# Function to check if the i'th league is already over or not to avoid playing the league again by checking if the league number is present in the dictionary of leage_results.json file
def check_league(league_no):
    
        """
        Parameters
        ----------
        league_no: int
            The league number to be checked if it is already over or not
    
        Returns
        --------
        Bool: True if the league is already over, False otherwise
    
        """

        if os.path.exists('league_results.json') == False:
            return False
        
        # Load the dictionary from the JSON file
        with open('league_results.json', 'r') as json_file:
            data = json.load(json_file)
        
        # Check if the league number is present in the dictionary
        if str(league_no) in data:
            return True
        else:
            return False


# Function which takes all the prompts and objects as input and plays a round robin league and updates the elo ratings of the prompts and returns the top 10 and bottom 10 prompts after the league
def league(prompts, questions, answers, k, league_no, temp, max_tokens, human = False, human_file = None):

    """
    Parameters
    ----------
    prompts: List
        List of all the prompts to be used in the league
    objects: List
        List of all the objects to be used in the league
    k: int
        Number of top and bottom prompts to be returned
    league_no: int
        The league number for which the league is to be
    human: bool
        True if the prompts playing the league are given by humans, False if the prompts are generated by LLM
    human_file: str
        The name of the file where the human prompts are stored
    final: bool
        True if it is the final league, False otherwise

        
    Returns
    --------
    top_10_prompts: List
        List of the top 10 prompts with the highest ELO ratings after the league
    bottom_10_prompts: List
        List of the bottom 10 prompts with the lowest ELO ratings after the league
    """

    global elo_ratings_dict
    global league_dict
    global top_bottom_prompts_dict
    global human_prompts_used_dict
    global origin_league_prompts_dict

    for prompt in prompts:
        origin_league_prompts_dict[prompt] = league_no

    #Check if the league is already palyed or not
    if check_league(league_no):

        print(f"League {league_no} is already over!")
        # Get the top and bottom k prompts along with their final elo scores from the league_results.json file for that league_no and also put it in the top_bottom_prompts_dict
        with open('league_results.json', 'r') as json_file:
            data = json.load(json_file)
        
        # Collect all prompts with their final ELO scores
        all_prompts = {}
        for battle_no, battle_dict in data[str(league_no)].items():
            prompt_1 = battle_dict['prompt_1']
            prompt_2 = battle_dict['prompt_2']
            all_prompts[prompt_1['prompt']] = prompt_1['final_elo']
            all_prompts[prompt_2['prompt']] = prompt_2['final_elo']


        # Store all the prompts in the human_prompts_used_dict if human prompts are used
        if human == True and human_file != None:
            human_prompts_used_dict[league_no] = all_prompts.keys()

        # Store all the elo scores in the elo_ratings_dict with the key has prompt and value as elo score
        for prompt, elo in all_prompts.items():
            elo_ratings_dict[prompt] = elo

        # Sort prompts by ELO score
        sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1], reverse=True)

        # Get the top k and bottom k prompts
        top_k_prompts = sorted_prompts[:k]
        bottom_k_prompts = sorted_prompts[-k:]

        # Save the top and bottom k prompts in the dictionary for the league and also rank them by 1,2,3..,k
        top_bottom_prompts_dict[league_no] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
        }

    # Dump the top and bottom k prompts in a json file using the top_bottom_prompts_dict
        with open('top_bottom_prompts_dict.json', 'w') as json_file:
            json.dump(top_bottom_prompts_dict, json_file, indent=4)

        return top_k_prompts, bottom_k_prompts
    
    temp_league_dict = {}
    temp_league_dict[league_no] = {}


    for prompt in prompts:
        elo_ratings_dict[prompt] = 1200

    battle_no = 0

    round_robin_list = [(i, j) for i in range(len(prompts)) for j in range(i + 1, len(prompts))]
    random.shuffle(round_robin_list)

    # To store results for ELO updates after parallel processing
    results_list = [None] * len(round_robin_list)

    start_time = time.time()

    # Use ThreadPoolExecutor to run games in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
        future_to_battle = {}

        for index, (i, j) in enumerate(round_robin_list):
            # Select the same question and answer for both prompts in this battle
            question = random.choice(questions)
            answer = answers[questions.index(question)]
            
            # Submit two tasks: one for each prompt in the battle with the same question and answer
            future_1 = executor.submit(game, question, answer, prompts[i], temp, max_tokens)
            future_2 = executor.submit(game, question, answer, prompts[j], temp, max_tokens)
            
            # Store both futures with their respective indices
            future_to_battle[future_1] = (index, i, j, 'first')
            future_to_battle[future_2] = (index, i, j, 'second')

        # Process results as they complete
        results_dict = {}  # Dictionary to hold results temporarily
        for future in concurrent.futures.as_completed(future_to_battle):
            index, i, j, which = future_to_battle[future]
        
            result = future.result()
            if which == 'first':
                if (index, i, j) in results_dict:
                    # print(index, i, j)
                    results_dict[(index, i, j)][0] = result
                else:
                    # print(index, i, j)
                    results_dict[(index, i, j)] = [result, None]
            else:
                if (index, i, j) in results_dict:
                    # print(index, i, j)
                    results_dict[(index, i, j)][1] = result  # Store result_2
                else:
                    # print(index, i, j)
                    results_dict[(index, i, j)] = [None, result]
        
        # Convert dictionary to list to maintain order
        results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

    # Process results in order of the original round robin list
    for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
        
    
        question = result_1[0]
        # Find the answer from the answers list for the corresponding question
        answer = answers[questions.index(question)]
        final_answer = result_1[2]

        # Get initial ELO ratings
        initial_elo_rating_1 = elo_ratings_dict[prompts[i]]
        initial_elo_rating_2 = elo_ratings_dict[prompts[j]]

        # Update ELO ratings using elo_rating function
        _, _, winner = elo_rating(prompts[i], prompts[j], question, answer, temp, max_tokens, game_over=True, result_1=result_1, result_2=result_2)
        
        # Get final ELO ratings
        final_elo_rating1 = elo_ratings_dict[prompts[i]]
        final_elo_rating2 = elo_ratings_dict[prompts[j]]

        # Increment battle number and prepare battle dictionary
        battle_no += 1
        battle_dict = {
            'question': question,
            'final_answer': final_answer,
            'prompt_1': {
                'prompt': prompts[i],
                'solution': result_1[3] if result_1 else "",
                'final_answer_given': result_1[4] if result_1 else "",
                'answered_correctly': result_1[5] if result_1 else False,
                'initial_elo': initial_elo_rating_1,
                'final_elo': final_elo_rating1
            },
            'prompt_2': {
                'prompt': prompts[j],
                'solution': result_2[3] if result_2 else "",
                'final_answer_given': result_2[4] if result_2 else "",
                'answered_correctly': result_2[5] if result_2 else False,
                'initial_elo': initial_elo_rating_2,
                'final_elo': final_elo_rating2
            },
            'winner': winner
        }

        temp_league_dict[league_no][battle_no] = battle_dict

    end_time = time.time()

    print(f"Time taken to complete the league: {end_time - start_time} seconds")
    
    # Store the prompts used in the human_prompts_used_dict if human prompts are used
    if human == True and human_file != None:
        human_prompts_used_dict[league_no] = prompts


    temp_elo_dict={}
    for i in range(len(prompts)):
        temp_elo_dict[prompts[i]]=elo_ratings_dict[prompts[i]]

    sorted_prompts = sorted(temp_elo_dict.items(), key=lambda x: x[1], reverse=True)

    # Get the top k and bottom k prompts
    top_k_prompts = sorted_prompts[:k]
    bottom_k_prompts = sorted_prompts[-k:]
    
    # Check if the JSON file exists
    file_path = 'league_results.json'
    if os.path.exists(file_path):
        # If the file exists, load the existing data
        with open(file_path, 'r') as json_file:
            data = json.load(json_file)
    else:
        # If the file does not exist, start with an empty dictionary
        data = {}

    # Update the data with the current league_dict
    data.update(temp_league_dict)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)
    
    # Save the top and bottom k prompts in the dictionary for the league and also rank them by 1,2,3..,k
    top_bottom_prompts_dict[league_no] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
    }

    # Dump the top and bottom k prompts in a json file using the top_bottom_prompts_dict
    with open('top_bottom_prompts_dict.json', 'w') as json_file:
        json.dump(top_bottom_prompts_dict, json_file, indent=4)

    # # Load the dictionary from the pickle file
    # with open('league_results.pkl', 'rb') as pickle_file:
    #     league_dict = pickle.load(pickle_file)

    # return top and bottom k prompts
    return top_k_prompts, bottom_k_prompts


# Function which takes all the prompts and objects as input and plays a round robin league and updates the elo ratings of the prompts and returns the top 10 and bottom 10 prompts after the league
def final_league(prompts, questions, answers, k, league_no, temp, max_tokens):

    global top_bottom_prompts_dict_across_league

    global elo_ratings_dict
    global league_dict
    global top_bottom_prompts_dict
    global human_prompts_used_dict
    global origin_league_prompts_dict

    final_elo_dict = {}
    
    if os.path.exists(f'final_league_results{league_no}.json'):
        print(f"Final League {league_no} is already over!")

        # Get the top and bottom k prompts along with their final elo scores from the league_results.json file for that league_no and also put it in the top_bottom_prompts_dict
        with open(f'final_league_results{league_no}.json', 'r') as json_file:
            data = json.load(json_file)
        
        # Collect all prompts with their final ELO scores
        all_prompts = {}
        for battle_no, battle_dict in data[str(league_no)].items():
            prompt_1 = battle_dict['prompt_1']
            prompt_2 = battle_dict['prompt_2']
            all_prompts[prompt_1['prompt']] = prompt_1['final_elo']
            all_prompts[prompt_2['prompt']] = prompt_2['final_elo']


        # Store all the elo scores in the elo_ratings_dict with the key has prompt and value as elo score
        for prompt, elo in all_prompts.items():
            final_elo_dict[prompt] = elo

        # Sort prompts by ELO score
        sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1], reverse=True)

        # Get the top k and bottom k prompts
        top_k_prompts = sorted_prompts[:k]
        bottom_k_prompts = sorted_prompts[-k:]

        # Store the top_k_prompts, bottom_k_prompts in top_bottom_prompts_dict_across_league
        top_bottom_prompts_dict_across_league[league_no] = {
            'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
            'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
        }

        #Dump the top and bottom k prompts in a json file using the top_bottom_prompts_dict_across_league
        with open('top_bottom_prompts_dict_across_league.json', 'w') as json_file:
            json.dump(top_bottom_prompts_dict_across_league, json_file, indent=4)

        return top_k_prompts, bottom_k_prompts, final_elo_dict
    
    temp_league_dict = {}
    temp_league_dict[league_no] = {}

    for prompt in prompts:
        final_elo_dict[prompt] = 1200

    battle_no = 0

    round_robin_list = [(i, j) for i in range(len(prompts)) for j in range(i + 1, len(prompts))]
    random.shuffle(round_robin_list)

    # To store results for ELO updates after parallel processing
    results_list = [None] * len(round_robin_list)

    start_time = time.time()

    # Use ThreadPoolExecutor to run games in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
        future_to_battle = {}

        for index, (i, j) in enumerate(round_robin_list):
            # Select the same question and answer for both prompts in this battle
            question = random.choice(questions)
            answer = answers[questions.index(question)]
            
            # Submit two tasks: one for each prompt in the battle with the same question and answer
            future_1 = executor.submit(game, question, answer, prompts[i], temp, max_tokens)
            future_2 = executor.submit(game, question, answer, prompts[j], temp, max_tokens)
            
            # Store both futures with their respective indices
            future_to_battle[future_1] = (index, i, j, 'first')
            future_to_battle[future_2] = (index, i, j, 'second')

        # Process results as they complete
        results_dict = {}  # Dictionary to hold results temporarily
        for future in concurrent.futures.as_completed(future_to_battle):
            index, i, j, which = future_to_battle[future]
            result = future.result()
            if which == 'first':
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][0] = result
                else:
                    results_dict[(index, i, j)] = [result, None]
            else:
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][1] = result  # Store result_2
                else:
                    results_dict[(index, i, j)] = [None, result]

        # Convert dictionary to list to maintain order
        results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

    # Process results in order of the original round robin list
    for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
        
        question = result_1[0]
        # Find the answer from the answers list for the corresponding question
        answer = answers[questions.index(question)]
        final_answer = result_1[2]

        # Get initial ELO ratings
        initial_elo_rating_1 = final_elo_dict[prompts[i]]
        initial_elo_rating_2 = final_elo_dict[prompts[j]]

        # Update ELO ratings using elo_rating function
        _, _, winner = elo_rating(prompts[i], prompts[j], question, answer, temp, max_tokens, game_over=True, result_1=result_1, result_2=result_2, custom_dict= final_elo_dict)
        
        # Get final ELO ratings
        final_elo_rating1 = final_elo_dict[prompts[i]]
        final_elo_rating2 = final_elo_dict[prompts[j]]

        # Increment battle number and prepare battle dictionary
        battle_no += 1
        battle_dict = {
            'question': question,
            'final_answer': final_answer,
            'prompt_1': {
                'prompt': prompts[i],
                'solution': result_1[3] if result_1 else "",
                'final_answer_given': result_1[4] if result_1 else "",
                'answered_correctly': result_1[5] if result_1 else False,
                'initial_elo': initial_elo_rating_1,
                'final_elo': final_elo_rating1
            },
            'prompt_2': {
                'prompt': prompts[j],
                'solution': result_2[3] if result_2 else "",
                'final_answer_given': result_2[4] if result_2 else "",
                'answered_correctly': result_2[5] if result_2 else False,
                'initial_elo': initial_elo_rating_2,
                'final_elo': final_elo_rating2
            },
            'winner': winner
        }

        temp_league_dict[league_no][battle_no] = battle_dict

    end_time = time.time()

    print(f"Time taken to complete the league: {end_time - start_time} seconds")


    temp_elo_dict={}
    for i in range(len(prompts)):
        temp_elo_dict[prompts[i]]=final_elo_dict[prompts[i]]

    sorted_prompts = sorted(temp_elo_dict.items(), key=lambda x: x[1], reverse=True)

    # Get the top k and bottom k prompts
    top_k_prompts = sorted_prompts[:k]
    bottom_k_prompts = sorted_prompts[-k:]
    
    # Check if the JSON file exists
    file_path = f'final_league_results{league_no}.json'
    
    data = {}

    # Update the data with the current league_dict
    data.update(temp_league_dict)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)
    

    # # Load the dictionary from the pickle file
    # with open('league_results.pkl', 'rb') as pickle_file:
    #     league_dict = pickle.load(pickle_file)

    # return top and bottom k prompts

    # Store the top_k_prompts, bottom_k_prompts in top_bottom_prompts_dict_across_league
    top_bottom_prompts_dict_across_league[league_no] = {
            'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
            'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
    }

    #Dump the top and bottom k prompts in a json file using the top_bottom_prompts_dict_across_league
    with open('top_bottom_prompts_dict_across_league.json', 'w') as json_file:
        json.dump(top_bottom_prompts_dict_across_league, json_file, indent=4)

    return top_k_prompts, bottom_k_prompts, final_elo_dict


def compare_elo_average(top_prompts_league, final_elo_dict, league_no, k, consecutive_fails):

    # Get the elo average of top_prompts_league using the final_elo_dict
    elo_average_league = 0
    for prompt in top_prompts_league:
        elo_average_league += final_elo_dict[prompt]
    elo_average_league = elo_average_league/len(top_prompts_league)

    # Get the top k prompts from the top_bottom_prompts_dict_across_league.json file for that league_no -1
    with open('top_bottom_prompts_dict_across_league.json', 'r') as json_file:
        data = json.load(json_file)

    top_prompts_final = data[str(league_no-1)]['top_k_prompts']
    top_prompts_final = [top_prompts_final[str(i)]['prompt'] for i in range(1, k+1)]

    elo_average_final = 0

    for prompt in top_prompts_final:
        elo_average_final += final_elo_dict[prompt]
    elo_average_final = elo_average_final/len(top_prompts_final)

    if (elo_average_final > elo_average_league):
        consecutive_fails +=1
        print("consecutive_fails: ", consecutive_fails)
        
    else:
        consecutive_fails = 0
        print("consecutive_fails: ", consecutive_fails)

    return consecutive_fails



def proof_of_concept(league1, league2, questions, answers,  k , temp, max_tokens):

    global top_bottom_prompts_dict_across_league
    global top_bottom_prompts_dict
    global origin_league_prompts_dict
    global top_bottom_prompts_dict_poc

    league_no = 1

    # Select 10 random topics from the topics list
    questions = random.sample(questions, 100)
    answers = [answers[questions.index(question)] for question in questions]

    poc_elo_dict = {}

    if league1 !=1:
        # Get pro_prompts1 by getting top k prompts from the league1 using top_bottom_prompts_dict_across_league
        top_prompts_league1 = top_bottom_prompts_dict_across_league[league1]['top_k_prompts']
        prompts1 = [top_prompts_league1[i]['prompt'] for i in range(1, k+1)]
    
        # Get pro_prompts2 by getting top k prompts from the league2 using top_bottom_prompts_dict_across_league
        top_prompts_league2 = top_bottom_prompts_dict_across_league[league2]['top_k_prompts']
        prompts2 = [top_prompts_league2[i]['prompt'] for i in range(1, k+1)]

    if league1 == 1:

        # Get the pro_prompts1 by getting top k prompts from the league1 using top_bottom_prompts_dict
        top_prompts_league1 = top_bottom_prompts_dict[league1]['top_k_prompts']
        prompts1 = [top_prompts_league1[i]['prompt'] for i in range(1, k+1)]

        # Get the pro_prompts2 by getting top k prompts from the league2 using top_bottom_prompts_dict_across_league
        top_prompts_league2 = top_bottom_prompts_dict_across_league[league2]['top_k_prompts']
        prompts2 = [top_prompts_league2[i]['prompt'] for i in range(1, k+1)]

    # Merge the pro_prompts1 and pro_prompts2 to get the pro_prompts
    prompts = prompts1 + prompts2

    #Check if the file exists
    if os.path.exists(f'proof_of_concept_{league1}_{league2}.json'):
        print(f"Proof of Concept {league1} vs {league2} is already over!")
        # Get the top and bottom k prompts for pro and con side along with their final elo scores from the proof_of_concept_{league1}_{league2}.json' file for that league_no
        with open(f'proof_of_concept_{league1}_{league2}.json', 'r') as json_file:
            data = json.load(json_file)

        all_prompts = {}
        for battle_no, battle_dict in data[str(league_no)].items():
            prompt_1 = battle_dict['prompt_1']
            prompt_2 = battle_dict['prompt_2']
            all_prompts[prompt_1['prompt']] = prompt_1['final_elo']
            all_prompts[prompt_2['prompt']] = prompt_2['final_elo']

        # Store all the elo scores in the elo_ratings_dict with the key has prompt and value as elo score
        for prompt, elo in all_prompts.items():
            poc_elo_dict[prompt] = elo

        # Sort prompts by ELO score
        sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1], reverse=True)

        # Get the top k and bottom k prompts
        top_k_prompts = sorted_prompts[:k]
        bottom_k_prompts = sorted_prompts[-k:]

        # Store the top and bototm k prompts in top_bottom_prompts_dict_poc along with origin
        top_bottom_prompts_dict_poc[f"{league1} vs {league2}"] = {
            'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
            'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
        }

        # Dump the top and bottom k prompts in a json file using the top_bottom_prompts_dict_poc
        with open('top_bottom_prompts_dict_poc.json', 'w') as json_file:
            json.dump(top_bottom_prompts_dict_poc, json_file, indent=4)

        compare_poc_elo_average(poc_elo_dict, prompts1, prompts2, league1, league2)

        return
    
    temp_league_dict = {}
    temp_league_dict[league_no] = {}

    
    for prompt in prompts:
        poc_elo_dict[prompt] = 1200

    battle_no = 0

    round_robin_list = [(i, j) for i in range(len(prompts)) for j in range(i + 1, len(prompts))]
    random.shuffle(round_robin_list)

    # To store results for ELO updates after parallel processing
    results_list = [None] * len(round_robin_list)

    start_time = time.time()

    # Use ThreadPoolExecutor to run games in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
        future_to_battle = {}

        for index, (i, j) in enumerate(round_robin_list):
            # Select the same question and answer for both prompts in this battle
            question = random.choice(questions)
            answer = answers[questions.index(question)]
            
            # Submit two tasks: one for each prompt in the battle with the same question and answer
            future_1 = executor.submit(game, question, answer, prompts[i], temp, max_tokens)
            future_2 = executor.submit(game, question, answer, prompts[j], temp, max_tokens)
            
            # Store both futures with their respective indices
            future_to_battle[future_1] = (index, i, j, 'first')
            future_to_battle[future_2] = (index, i, j, 'second')

        # Process results as they complete
        results_dict = {}  # Dictionary to hold results temporarily
        for future in concurrent.futures.as_completed(future_to_battle):
            index, i, j, which = future_to_battle[future]
           
            result = future.result()
            if which == 'first':
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][0] = result
                else:
                    results_dict[(index, i, j)] = [result, None]
            else:
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][1] = result  # Store result_2
                else:
                    results_dict[(index, i, j)] = [None, result]
            
        # Convert dictionary to list to maintain order
        results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

    # Process results in order of the original round robin list
    for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
        
        question = result_1[0]
        # Find the answer from the answers list for the corresponding question
        answer = answers[questions.index(question)]
        final_answer = result_1[2]

        # Get initial ELO ratings
        initial_elo_rating_1 = poc_elo_dict[prompts[i]]
        initial_elo_rating_2 = poc_elo_dict[prompts[j]]

        # Update ELO ratings using elo_rating function
        _, _, winner = elo_rating(prompts[i], prompts[j], question, answer, temp, max_tokens, game_over=True, result_1=result_1, result_2=result_2, custom_dict= poc_elo_dict)
        
        # Get final ELO ratings
        final_elo_rating1 = poc_elo_dict[prompts[i]]
        final_elo_rating2 = poc_elo_dict[prompts[j]]

        # Increment battle number and prepare battle dictionary
        battle_no += 1
        battle_dict = {
            'question': question,
            'final_answer': final_answer,
            'prompt_1': {
                'prompt': prompts[i],
                'solution': result_1[3] if result_1 else "",
                'final_answer_given': result_1[4] if result_1 else "",
                'answered_correctly': result_1[5] if result_1 else False,
                'initial_elo': initial_elo_rating_1,
                'final_elo': final_elo_rating1
            },
            'prompt_2': {
                'prompt': prompts[j],
                'solution': result_2[3] if result_2 else "",
                'final_answer_given': result_2[4] if result_2 else "",
                'answered_correctly': result_2[5] if result_2 else False,
                'initial_elo': initial_elo_rating_2,
                'final_elo': final_elo_rating2
            },
            'winner': winner
        }

        temp_league_dict[league_no][battle_no] = battle_dict

    end_time = time.time()

    print(f"Time taken to complete the league: {end_time - start_time} seconds")


    temp_elo_dict={}
    for i in range(len(prompts)):
        temp_elo_dict[prompts[i]]=poc_elo_dict[prompts[i]]

    sorted_prompts = sorted(temp_elo_dict.items(), key=lambda x: x[1], reverse=True)

    # Get the top k and bottom k prompts
    top_k_prompts = sorted_prompts[:k]
    bottom_k_prompts = sorted_prompts[-k:]
    
    # Check if the JSON file exists
    file_path = f'proof_of_concept_{league1}_{league2}.json'
    
    data = {}

    # Update the data with the current league_dict
    data.update(temp_league_dict)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)

    # Store the top and bototm k prompts in top_bottom_prompts_dict_poc along with origin
    top_bottom_prompts_dict_poc[f"{league1} vs {league2}"] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
    }

    # Dump the top and bottom k prompts in a json file using the top_bottom_prompts_dict_poc
    with open('top_bottom_prompts_dict_poc.json', 'w') as json_file:
        json.dump(top_bottom_prompts_dict_poc, json_file, indent=4)


    compare_poc_elo_average(poc_elo_dict, prompts1, prompts2, league1, league2)

    return

def compare_poc_elo_average(poc_elo_dict, prompts1, prompts2, league1, league2):
    # Get the average of prompts1, prompts2,  using the poc_elo_dict
    elo_average_prompts1 = 0
    for prompt in prompts1:
        elo_average_prompts1 += poc_elo_dict[prompt]
    elo_average_prompts1 = elo_average_prompts1/len(prompts1)
    print("elo_average_prompts1: ", elo_average_prompts1)
    elo_average_prompts2 = 0
    for prompt in prompts2:
        elo_average_prompts2 += poc_elo_dict[prompt]
    elo_average_prompts2 = elo_average_prompts2/len(prompts2)
    print("elo_average_prompts2: ", elo_average_prompts2)

    if (elo_average_prompts1 > elo_average_prompts2):
        print(f"Proof of Concept {league1} vs {league2} Failed!")
    else:
        print(f"Proof of Concept {league1} vs {league2} Passed!")

#Function to play the tournament. In this, first we play the initial league on the prompts generated by the get_prompts() function and then we get the top and bottom k prompts. Then we generate new prompts using the get_new_prompts() function and play the league on these new prompts. We repeat this process for n-1 number of leagues. We give the top and bottom k prompts of current league as input to the get_new_prompts() function to generate new prompts for the next league. Then we play the n'th league which is the final league which takes the top k prompts and bottom k from each of the n-1 leagues and conducts a final league on these prompts. The top k prompts from this final league are the top k prompts of the tournament.
def tournament(no_of_questions, no_of_prompts_start, no_of_prompts_between, k, n, temp, max_tokens, current_directory):
    
        """
        Parameters
        ----------
        no_of_objects: int
            Number of objects required for the 20 questions game
        no_of_prompts_start: int
            Number of prompts required for the 20 questions game for the first league
        no_of_prompts_between: int
            Number of prompts required for the 20 questions game for the leagues in between
        k: int
            Number of top and bottom prompts to be returned
        n: int
            Number of leagues to be played
        
        Returns
        --------
        None
        """

        global top_bottom_prompts_dict
        global elo_ratings_dict
        global current_league

        file_path = os.path.join(current_directory, 'humans.csv')
        questions, answers = load_dataset_from_huggingface(no_of_questions)
        consecutive_fails = 0
        # Play all the leagues
        while(consecutive_fails<3):
            print(f"League {current_league}:\n")
            # Play the first league
            if (current_league ==1):

                prompts = get_prompts(no_of_prompts_start, current_league, temp, max_tokens, None, None, True, file_path)
    
                if prompts is None:
                    print("Invalid Input")
                    return
                
                top_k_prompts_league1, bottom_k_prompts_league1 = league(prompts, questions, answers, k, current_league, temp, max_tokens, True, file_path)
                # Get only the prompts from the top_k_prompts_league1 and bottom_k_prompts_league1
                top_k_prompts_league1 = [prompt for prompt, elo in top_k_prompts_league1]
                bottom_k_prompts_league1 = [prompt for prompt, elo in bottom_k_prompts_league1]
                current_league+=1

            elif (current_league !=1):

                if(current_league==6):
                    break

                if (current_league ==2):
                # Generate new prompts for the next league using the top k and bottom k prompts from the previous league
                    prompts = get_prompts(no_of_prompts_between, current_league, temp, max_tokens, top_k_prompts_league1, bottom_k_prompts_league1, False, None)

                    if prompts is None:
                        print("Invalid Input")
                        return

                # Play the league on the new prompts
                    top_prompts_league, bottom_prompts_league = league(prompts, questions, answers, k, current_league, temp, max_tokens, False, None)
                    
                    # Get only the prompts from the top_prompts_league, bottom_prompts_league
                    top_prompts_league = [prompt for prompt, elo in top_prompts_league]
                    bottom_prompts_league = [prompt for prompt, elo in bottom_prompts_league]
                    

                    # Merge the top prompts of league 1 and 2 into a new list
                    merged_top_prompts = top_k_prompts_league1 + top_prompts_league
                    merged_bottom_prompts = bottom_k_prompts_league1 + bottom_prompts_league
                    

                # Play the final league using the merged top prompts from league 1 and 2
                    top_prompts_final, bottom_prompts_final, final_elo_dict = final_league(merged_top_prompts, questions, answers, k, current_league, temp, max_tokens)

                    # Get only the prompts from the top_prompts_final, bottom_prompts_final
                    top_prompts_final = [prompt for prompt, elo in top_prompts_final]
                    bottom_prompts_final = [prompt for prompt, elo in bottom_prompts_final]

                    current_league+=1
                
                elif(current_league>2):

                    # Merge the top pro prompts of final
                    prompts = get_prompts(no_of_prompts_between, current_league, temp, max_tokens, top_prompts_final, bottom_prompts_league, False, None)
                   

                    if prompts is None:
                        print("Invalid Input")
                        return
                    
                    top_prompts_league, bottom_prompts_league = league(prompts, questions, answers, k, current_league, temp, max_tokens, False, None)

                    # Get only the prompts from the top_prompts_league, bottom_prompts_league
                    top_prompts_league = [prompt for prompt, elo in top_prompts_league]
                    bottom_prompts_league = [prompt for prompt, elo in bottom_prompts_league]
                    
                    # Merge the top and bottom prompts of league and final
                    merged_top_prompts = top_prompts_league + top_prompts_final
                    merged_bottom_prompts = bottom_prompts_league + bottom_prompts_final
                    
                    # Play the final league using the merged top pro and con prompts from league and final
                    top_prompts_final, bottom_prompts_final, final_elo_dict = final_league(merged_top_prompts, questions, answers, k, current_league, temp, max_tokens)

                    # Get only the prompts from the top_prompts_final, bottom_prompts_final
                    top_prompts_final = [prompt for prompt, elo in top_prompts_final]
                    bottom_prompts_final = [prompt for prompt, elo in bottom_prompts_final]

                    consecutive_fails = compare_elo_average(top_prompts_league, final_elo_dict, current_league, k , consecutive_fails)

                    if (consecutive_fails>=3):
                        break
                
                    current_league+=1

        
        # Proof of concept run
        last_league_no = 2
        # Find last league number using prompts{last_league_no}.json file
        while(f'prompts_{last_league_no}.csv' in os.listdir()):
            last_league_no+=1
        last_league_no-=1
        
        proof_of_concept(1, last_league_no, questions, answers, k, temp, max_tokens)
        proof_of_concept(2, last_league_no, questions, answers, k, temp, max_tokens)

    
# # Function to run the final league for all the values less than to n by extracting top k prompts from previous leagues and playing the final league
# def run_final_leagues(no_of_questions, n, k, temp, max_tokens):

#     # Read the league_results.json file
#     with open('league_results.json', 'r') as json_file:
#         data = json.load(json_file)
        
#     questions, answers = load_dataset_from_huggingface(no_of_questions)

#     # Play all the final leagues from 3 to n-1
#     for z in range(3, n):
#         specific_elo_ratings_dict = {}
#         league_dictionary = {}
#         top_prompts = []
#         league_number = z
#         # Check if the league is already played or not
#         if (f'final_league_results{z}.json') in os.listdir():
#             print(f"Final League {z} is already over!")
#             continue
#         league_dictionary[league_number] = {}
#         battle_number = 0

#         # Collect all the questoner prompts from the previous leagues of i using league_resutls.json file along with their elo scores
#         all_prompts = {}

#         for league_no in range(1, z):
#             all_prompts[league_no] = {}
#             for battle_no, battle_dict in data[str(league_no)].items():
#                 prompt_1 = battle_dict['prompt_1']['prompt']
#                 prompt_2 = battle_dict['prompt_2']['prompt']
#                 prompt_1_elo = battle_dict['prompt_1']['final_elo']
#                 prompt_2_elo = battle_dict['prompt_2']['final_elo']
#                 all_prompts[league_no][prompt_1] = prompt_1_elo
#                 all_prompts[league_no][prompt_2] = prompt_2_elo
#                 # Store the prompt and elo in the specific_elo_ratings_dict
#             sorted_prompts = sorted(all_prompts[league_no].items(), key=lambda x: x[1], reverse=True)
#             top_k_prompts = sorted_prompts[:k]
#             for prompt, elo in top_k_prompts:
#                 specific_elo_ratings_dict[prompt] = elo
#                 top_prompts.append(prompt)
    
#         # Generate the round robin list with pair (i, j) where i is the indice of pro_prompt and j is the indice of con_prompt
#         round_robin_list = [(i, j) for i in range(len(top_prompts)) for j in range(i + 1, len(top_prompts))]
#         random.shuffle(round_robin_list)

#         # To store results for ELO updates after parallel processing
#         results_list = [None] * len(round_robin_list)

#         # Use ThreadPoolExecutor to run games in parallel
#         with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
#             future_to_battle = {}

#             for index, (i, j) in enumerate(round_robin_list):
#                 # Select the same question and answer for both prompts in this battle
#                 question = random.choice(questions)
#                 answer = answers[questions.index(question)]
                
#                 # Submit two tasks: one for each prompt in the battle with the same question and answer
#                 future_1 = executor.submit(game, question, answer, top_prompts[i], temp, max_tokens)
#                 future_2 = executor.submit(game, question, answer, top_prompts[j], temp, max_tokens)
                
#                 # Store both futures with their respective indices
#                 future_to_battle[future_1] = (index, i, j, 'first')
#                 future_to_battle[future_2] = (index, i, j, 'second')

#             # Process results as they complete
#             results_dict = {}  # Dictionary to hold results temporarily
#             for future in concurrent.futures.as_completed(future_to_battle):
#                 index, i, j, which = future_to_battle[future]
#                 try:
#                     result = future.result()
#                     if which == 'first':
#                         if (index, i, j) in results_dict:
#                             results_dict[(index, i, j)][0] = result
#                         else:
#                             results_dict[(index, i, j)] = [result, None]
#                     else:
#                         if (index, i, j) in results_dict:
#                             results_dict[(index, i, j)][1] = result  # Store result_2
#                         else:
#                             results_dict[(index, i, j)] = [None, result]
#                 except Exception as exc:
#                     print(f'Game {i} vs {j} ({which}) generated an exception: {exc}')
#                     results_dict[(index, i, j)] = [None, None]  # In case of failure, store None

#             # Convert dictionary to list to maintain order
#             results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

#         # Process results in order of the original round robin list
#         for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
            
#             question = result_1[0]
#             # Find the answer from the answers list for the corresponding question
#             answer = answers[questions.index(question)]

#             # Get initial ELO ratings
#             initial_elo_rating_1 = specific_elo_ratings_dict[top_prompts[i]]
#             initial_elo_rating_2 = specific_elo_ratings_dict[top_prompts[j]]

#             # Update ELO ratings using update_elo function
#             specific_elo_ratings_dict, winner = update_elo(specific_elo_ratings_dict, top_prompts[i], top_prompts[j], result_1, result_2)
            
#             # Get final ELO ratings
#             final_elo_rating1 = specific_elo_ratings_dict[top_prompts[i]]
#             final_elo_rating2 = specific_elo_ratings_dict[top_prompts[j]]

#             # Increment battle number and prepare battle dictionary
#             battle_number += 1
#             battle_dict = {
#                 'question': question,
#                 'final_answer': answer,
#                 'prompt_1': {
#                     'prompt': top_prompts[i],
#                     'solution': result_1[3] if result_1 else "",
#                     'final_answer_given': result_1[4] if result_1 else "",
#                     'answered_correctly': result_1[5] if result_1 else False,
#                     'initial_elo': initial_elo_rating_1,
#                     'final_elo': final_elo_rating1
#                 },
#                 'prompt_2': {
#                     'prompt': top_prompts[j],
#                     'solution': result_2[3] if result_2 else "",
#                     'final_answer_given': result_2[4] if result_2 else "",
#                     'answered_correctly': result_2[5] if result_2 else False,
#                     'initial_elo': initial_elo_rating_2,
#                     'final_elo': final_elo_rating2
#                 },
#                 'winner': winner
#             }
        
#             # Add the battle dictionary to the league dictionary
#             league_dictionary[league_number][battle_number] = battle_dict
    
#         # Store the league_dictionary in the "final_league_results"+"z".json file
#         with open(f'final_league_results{z}.json', 'w') as json_file:
#             json.dump(league_dictionary, json_file, indent=4)
        

# # Function to just change the elo ratings of the prompts and store them in the input dictionary
# def update_elo(dict, prompt_1, prompt_2, result_1, result_2):

#     """
#     Parameters
#     ----------
#     dict: dict
#         The dictionary where the elo ratings of the prompts are to be stored
#     pro_prompt: str
#         The prompt for the Pro side debater
#     con_prompt: str
#         The prompt for the Con side debater
#     winner: str
#         The winner of the Debate Battle
#     """


  
#     answered_correctly_1 = result_1[5]
#     answered_correctly_2 = result_2[5]
    
#     # If both the prompts have answered the question correctly, then the game is a draw
#     if answered_correctly_1 == True and answered_correctly_2 == True:
#         winner = "draw"
#         loser = "draw"
#     # If prompt_1 has answered the question correctly, then prompt_1 is the winner
#     elif answered_correctly_1 == True:
#         winner = prompt_1
#         loser = prompt_2
#     # If prompt_2 has answered the question correctly, then prompt_2 is the winner
#     elif answered_correctly_2 == True:
#         winner = prompt_2
#         loser = prompt_1

#     #If it is a draw, then calculate the ELO ratings of the prompts and print the updated ELO ratings
#     if winner == "draw":
#         # Find the ELO scores of the winner and loser
#         winner_rating = dict[prompt_1]
#         loser_rating = dict[prompt_2]
#         # Calculate the expected score of the winner and loser
#         expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
#         expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
#         # Update the ELO ratings of the winner and loser
#         winner_rating = winner_rating + 32 * (0.5 - expected_winner)
#         loser_rating = loser_rating + 32 * (0.5 - expected_loser)
#         # Update the ELO ratings in the dictionary
#         dict[prompt_1] = winner_rating
#         dict[prompt_2] = loser_rating
#         # Print the updated ELO ratings
#         print("Updated ELO Ratings:")
#         print(f"{prompt_1}: {winner_rating}")
#         print(f"{prompt_2}: {loser_rating}")

#     # Update the ELO ratings of the prompts if it is not a draw
#     elif winner != "draw":
#         # Find the ELO scores of the winner and loser
#         winner_rating = dict[winner]
#         loser_rating = dict[loser]
#         # Calculate the expected score of the winner and loser
#         expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
#         expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
#         # Update the ELO ratings of the winner and loser
#         winner_rating = winner_rating + 32 * (1 - expected_winner)
#         loser_rating = loser_rating + 32 * (0 - expected_loser)
#         # Update the ELO ratings in the dictionary
#         dict[winner] = winner_rating
#         dict[loser] = loser_rating
#         # Print the updated ELO ratings
#         print("Updated ELO Ratings:")
#         print(f"{winner}: {winner_rating}")
#         print(f"{loser}: {loser_rating}")

#     return dict, winner


def main():

    global elo_ratings_dict
    global league_dict
    global top_bottom_prompts_dict
    global human_prompts_used_dict
    global current_league
    global top_bottom_prompts_dict_across_league
    global origin_league_prompts_dict
    global top_bottom_prompts_dict_poc

    no_of_questions = 200
    current_directory = os.getcwd()
    temp = [0.5]
    max_tokens =[701]

    no_of_runs = 0

    for no_of_prompts_start in range (10,12,2):
        for no_of_prompts_between in range (8,10,2):
            for k in range (3,math.floor(no_of_prompts_between/2),1):
                for temperature in temp:
                    for max_token in max_tokens:
                        if no_of_runs == 1:
                            break
                        print("no_of_prompts_start: ", no_of_prompts_start)
                        print("no_of_prompts_between: ", no_of_prompts_between)
                        print("k: ", k)
                        print("temperature: ", temperature)
                        print("max_token: ", max_token)
                        folder_name = "no_of_prompts_start_"+str(no_of_prompts_start)+"_no_of_prompts_between_"+str(no_of_prompts_between)+"_k_"+str(k)+"_temperature_"+str(temperature)+"_max_token_"+str(max_token)
                        new_directory = os.path.join(current_directory, folder_name)

                        # Check if the directory is already present then don't make the directory again
                        if os.path.exists(new_directory):
                            print(f"Directory {new_directory} already exists!")
                            
                        else:
                            os.makedirs(new_directory, exist_ok=True)
                        
                        os.chdir(new_directory)
                        tournament(no_of_questions, no_of_prompts_start, no_of_prompts_between, k, 15, temperature, max_token, current_directory)
                        # Dictionary to store the ELO ratings of the prompts
                        elo_ratings_dict = {}
                        # Dictionary to store the history of the games in the leagues
                        league_dict = {}
                        #league number
                        current_league = 1
                        # Dictionary to store the top and bottom k prompts from each league
                        top_bottom_prompts_dict = {}
                        # Dictionary to store the human prompts which have been used in a league already
                        human_prompts_used_dict = {}
                        top_bottom_prompts_dict_across_league = {}
                        origin_league_prompts_dict = {}
                        top_bottom_prompts_dict_poc = {}

                        no_of_runs += 1


main()
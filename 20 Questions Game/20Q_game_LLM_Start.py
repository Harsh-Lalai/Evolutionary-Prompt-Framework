import boto3
from botocore.config import Config
import time
from openai import OpenAI
import csv
import os
import random
from random import sample
import json
import pickle
import multiprocessing
from multiprocessing import Pool
import traceback
import pandas as pd
import math
import concurrent.futures
from evaluate import load
bertscore = load("bertscore")
from fuzzywuzzy import fuzz

#-> Changing prompts to decrease errors
#-> Another function for cost estimation
#-> Parallel Runnning of the code
#-> Changing the structure of the game
# -> Checkpoint to not allow more than certain amount of API calls

# Set the API key from the environment variable
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


# session = boto3.Session(profile_name="AuraIntern") # use this if you are using Ada. 

# # session = boto3.Session()


# config = Config(

#     read_timeout=120, # corresponds to inference time limit set for Bedrock 

#     connect_timeout=120,

#     retries={

#         "max_attempts": 5,

#     },

# )


# bedrock = session.client(

#     service_name='bedrock-runtime',

#     region_name="us-east-1",

#     endpoint_url="https://bedrock-runtime.us-east-1.amazonaws.com",

#     config=config

# )

# Dictionary to store the ELO ratings of the prompts
elo_ratings_dict = {}
# Dictionary to store the history of the games in the leagues
league_dict = {}
#league number
current_league = 1
# Dictionary to store the top and bottom k prompts from each league
top_bottom_prompts_dict = {}
# Dictionary to store the human prompts which have been used in a league already
human_prompts_used_dict = {}
top_bottom_prompts_dict_across_league = {}
origin_league_prompts_dict = {}

top_bottom_prompts_dict_poc = {}


#Function for the API calls
def api_call_openai(model_name, prompt, temp, max_tokens):
    try:
        response = client.chat.completions.create(
            model= model_name,
            temperature= temp,
            messages=[{"role": "user", "content": prompt}],
            max_tokens= max_tokens
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return None

def api_call_openai_json(model_name, prompt, temp, max_tokens):
    try:
        response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "user", "content": prompt},
                    ],
                    response_format={"type": "json_object"},
                    temperature=temp,
                    max_tokens=max_tokens
                )
        return json.loads(response.choices[0].message.content.strip())
    except Exception as e:
        return None


def api_call_claude(model, prompt,temp = 0.5, max_tokens=2048):

    api_template = {

        "modelId": "anthropic.claude-3-sonnet-20240229-v1:0",

        "contentType": "application/json",

        "accept": "application/json",

        "body": ""

    }



    body = {

        "max_tokens":  max_tokens,

        "stop_sequences": ["\n\nHuman:"],

        "anthropic_version": "bedrock-2023-05-31",

        "temperature": 0.5,

        "messages": [{"role": "user", "content": prompt}]

    }



    api_template["body"] = json.dumps(body)



    success = False

    for i in range(5):

        try:

            response = bedrock.invoke_model(**api_template)

            success = True

            break

        except:

            

            traceback.print_exc()

            time.sleep(1)



    if success:

        response_body = json.loads(response.get('body').read())

        return response_body["content"][0]["text"].strip()

    else:

        print("***exception!!!!!")

        return ""
    



def get_prompts(no_of_prompts, league_no, temp, max_tokens, top_k_prompts=None, bottom_k_prompts=None, human=False, human_file=None):
    """
    Parameters
    ----------
    no_of_prompts: int
        Number of prompts required for the 20 questions game
    league_no: int
        The league number for which the prompts are to be generated
    top_k_prompts: list
        List of top k prompts from the previous league
    bottom_k_prompts: list
        List of bottom k prompts from the previous league
    human: bool
        True if the prompts are to be generated by humans, False if the prompts are to be generated by LLM
    human_file: str
        The name of the file where the human prompts are stored

    Returns
    --------
    prompts: list
        List of prompts generated by LLM for the 20 questions game to be given to the questioner for the game
    """
    
    global human_prompts_used_dict

    if human and human_file is None:
        print("Error in input! Please provide the human_file for the human prompts to be read.")
        return None
    
    if human:
        if(check_league(1)):

            with open('league_results.json', 'r') as json_file:
                data = json.load(json_file)
        
            # Collect all prompts with their final ELO scores
            prompts = set()
            for battle_no, battle_dict in data[str(league_no)].items():
                prompt_1 = battle_dict['prompt_1']
                prompt_2 = battle_dict['prompt_2']
                prompts.add(prompt_1['questioner_prompt'])
                prompts.add(prompt_2['questioner_prompt'])

            list(prompts)

            return prompts

        
        else:
            # Read the prompts from the human_file and store them in the prompts set
            prompts = set()
            with open(human_file, mode='r') as file:  
                csv_reader = csv.reader(file)
                for row in csv_reader:
                    if row[0] != "Prompt ID":
                        prompts.add(row[1])

            prompts = list(prompts)

            # Filter out prompts that have already been used
            prompts = [prompt for prompt in prompts if prompt not in human_prompts_used_dict.values()]

            # Randomly sample the required number of prompts
            prompts = sample(prompts, no_of_prompts)
            return prompts

    last_prompt_id = get_last_prompt_id(f"prompts_{league_no}.csv")
    
    existing_prompts = set()

    if os.path.exists(f"prompts_{league_no}.csv"):
        with open(f"prompts_{league_no}.csv", "r") as f:
            reader = csv.reader(f)
            for row in reader:
                if row[0] != "Prompt ID":  # Skip the header row
                    existing_prompts.add(row[1].strip())

    # if not top_k_prompts and not bottom_k_prompts:
    #     if len(existing_prompts)==0:
    #         instruction_for_prompts = """I am a Large Language Model playing the 20-question game. Help me make a prompt to win the game for my role as the questioner.

    #         Make a creative, optimal prompt to help a large language model be good in their role as a questioner.

    #         Note that the prompt you are generating is independent of the object which Questioner has to guess. You are only generating a prompt for the Questioner.

    #         Make sure that the response starts with 'Prompt: ' followed by the prompt text."""
            
    #     elif len(existing_prompts)!=0:
    #         instruction_for_prompts = f"""I am a Large Language Model playing the 20-question game. Help me make a prompt to excel in my role as the questioner.

    #         Make a creative, optimal prompt to help a large language model be good in their role as a questioner.

    #         Additionally, I already have a set of prompts that have been generated so far:

    #         Existing prompts:
    #         {existing_prompts}

    #         Do not combine, replicate, or produce anything similar to these existing prompts. Your goal is to create a completely new and unique prompt.

    #         Note that the prompt you are generating is independent of the object which Questioner has to guess. You are only generating a prompt for the Questioner.

    #         Make sure that the response starts with 'Prompt: ' followed by the prompt text."""
    # else:
    #     # Generate the instruction_for_prompts dynamically
    #     top_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(top_k_prompts)])
    #     bottom_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(bottom_k_prompts)])

    #     if len(existing_prompts) == 0:
    #         instruction_for_prompts = f"""I am a Large Language Model playing the 20-question game. Help me make a prompt to excel in my role as the questioner.

    #         To assist you in generating better prompts, here are some examples of the best and worst prompts from previous games:

    #         Best prompts:
    #         {top_prompts_text}

    #         Worst prompts:
    #         {bottom_prompts_text}

    #         Use these examples to understand which strategies have worked well and which have not. However, your goal is not to combine or replicate the top prompts. Instead, use this insight to create a completely new and unique prompt that can outperform the previous best examples.
            
    #         Note that the prompt you are generating is independent of the object which Questioner has to guess. You are only generating a prompt for the Questioner.

    #         Make sure that the response starts with 'Prompt: ' followed by the prompt text."""
        
    #     elif len(existing_prompts) != 0:
    #         instruction_for_prompts = f"""I am a Large Language Model playing the 20-question game. Help me make a prompt to excel in my role as the questioner.

    #         To assist you in generating better prompts, here are some examples of the best and worst prompts from previous games:

    #         Best prompts:
    #         {top_prompts_text}

    #         Worst prompts:
    #         {bottom_prompts_text}

    #         Additionally, I already have a set of prompts that have been generated so far:

    #         Existing prompts:
    #         {existing_prompts}

    #         Use the best and worst prompts to understand which strategies have worked well or poorly in the past. Your goal is not to combine, replicate, or produce anything similar to the existing or best prompts. Instead, use the insights from the best prompts to create a completely new and unique prompt that can outperform the previous best examples.

    #         Note that the prompt you are generating is independent of the object which Questioner has to guess. You are only generating a prompt for the Questioner.

    #         Make sure that the response starts with 'Prompt: ' followed by the prompt text."""
    

    prompts = []
    prompt_set = existing_prompts.copy()

    while not check_prompts(no_of_prompts, f"prompts_{league_no}.csv"):

        times_runned = 0
        candidate_prompts_set = set()

        while(len(candidate_prompts_set)<4):

            if(times_runned>10):
                print("Unable to generate prompts! Limit exceeded")
                break

            json_response = api_call_openai_json("gpt-4o-mini-2024-07-18", instruction_for_prompts(top_k_prompts, bottom_k_prompts, existing_prompts, candidate_prompts_set), temp, max_tokens)
            prompt = extract_prompt(json_response)
            if prompt is None:
                print(json_response)
                times_runned += 1
                print("Response did not contain the required keys, retrying...")
                continue
            if prompt not in prompt_set:
                candidate_prompts_set.add(prompt)
            else:
                print("Prompt already exists in the prompt set, retrying...")
                times_runned += 1
        
        best_prompt, reason = finding_best_prompt_from_candidates(existing_prompts, list(candidate_prompts_set), temp, max_tokens, top_k_prompts, bottom_k_prompts)

        if best_prompt not in prompt_set:
            prompt_set.add(best_prompt)
            existing_prompts.add(best_prompt)
            last_prompt_id += 1
            with open(f"prompts_{league_no}.csv", "a", newline="") as f:
                writer = csv.writer(f)
                if f.tell() == 0:
                    writer.writerow(["Prompt ID", "Prompt"])
                writer.writerow([last_prompt_id, best_prompt])
        else:
            print("Best prompt already exists in the prompt set, retrying...")
            times_runned += 1
    
    print("Prompts generated successfully!")
    with open(f"prompts_{league_no}.csv", mode='r') as file:  
        csv_reader = csv.reader(file)
        for row in csv_reader:
            if row[0] != "Prompt ID":
                prompts.append(row[1])
    print(prompts)
    # bert_score()
    return prompts
    
def bert_score():
    """
    Calculate the BERT score for the prompts generated using the prompts csv files and store it in a json file.
    The index of the prompt is used as the key in the JSON file.
    """
    # Calculate the BERT score for the prompts_{current_league}.csv file with each other 
    df = pd.read_csv(f"prompts_{current_league}.csv")
    
    prompts = df["Prompt"].to_list()
    

    # Function to check if a key exists in JSON file
    def key_exists_in_json(filename, key):
        if os.path.exists(filename):
            with open(filename, "r") as f:
                data = json.load(f)
            return str(key) in data
        return False

    # Process Prompts
    for i in range(len(prompts) - 1):
        references = prompts[i + 1:]
        predictions = [prompts[i]] * len(references)
        
        # Check if the key for this prompt already exists in the JSON file
        if key_exists_in_json(f"bert_score_{current_league}.json", i):
            print(f"Skipping calculation for prompt {i}, already exists in the file.")
            continue
        
        # Compute the BERT score
        results = bertscore.compute(predictions=predictions, references=references, lang="en")

        # Check if the JSON file exists and update it
        if os.path.exists(f"bert_score_{current_league}.json"):
            with open(f"bert_score_{current_league}.json", "r") as f:
                data = json.load(f)
            data[str(i)] = results  # Store the results under the index `i`
        else:
            data = {str(i): results}

        # Write back to the JSON file
        with open(f"bert_score_{current_league}.json", "w") as f:
            json.dump(data, f)
    

    # Process cross-league prompts
    for i in range(1, current_league):
        if os.path.exists(f"prompts_{current_league-i}.csv"):
            df_prev = pd.read_csv(f"prompts_{current_league-i}.csv")
            prompts_prev = df_prev["Prompt"].to_list()

            # Process cross-league pro prompts
            for j in range(len(prompts)):
                if key_exists_in_json(f"bert_score_{current_league}_{current_league-i}.json", j):
                    print(f"Skipping calculation for cross-league prompt {j}, already exists in the file.")
                    continue
                
                references = prompts_prev
                predictions = [prompts[j]] * len(references)
                
                results = bertscore.compute(predictions=predictions, references=references, lang="en")
                
                # Check if the JSON file exists and update it
                if os.path.exists(f"bert_score_{current_league}_{current_league-i}.json"):
                    with open(f"bert_score_{current_league}_{current_league-i}.json", "r") as f:
                        data = json.load(f)
                    data[str(j)] = results  # Store the results under the index `j`
                else:
                    data = {str(j): results}

                # Write back to the JSON file
                with open(f"bert_score_{current_league}_{current_league-i}.json", "w") as f:
                    json.dump(data, f)
            
    
# Function to extract prompt text from JSON response
def extract_prompt(json_response):
    """
    Extracts the prompt text from the JSON response if it meets all criteria.
    """
    # Define the required keys
    required_keys = ["Reason for diversity", "Is this diverse", "Reason for best prompt", "Is this the best prompt", "Prompt Text"]

    # Check if all required keys are present and their values are non-zero or non-empty
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the prompt text
        prompt_text = json_response["Prompt Text"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        return prompt_text
    else:
        # If any key is missing or has an empty/zero value, return None
        return None

def instruction_for_prompts(top_k_prompts, bottom_k_prompts, existing_prompts, candidates):
        
    existing_and_candidate_prompts = existing_prompts.union(candidates)
    existing_and_candidate_prompts_text = ""
    if len(existing_and_candidate_prompts) != 0:
        existing_and_candidate_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(existing_and_candidate_prompts)])
    if top_k_prompts is not None:
        top_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(top_k_prompts)])
    if bottom_k_prompts is not None:
        bottom_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(bottom_k_prompts)])

    if top_k_prompts is None and bottom_k_prompts is None:
        if len(existing_and_candidate_prompts) == 0:
            instructions = (
                "Help me create a prompt which I can provide to an AI model to be a questioner in the 20 Questions game.\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Your goal is to generate a completely new and unique prompt that offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all other prompts possible.\n"
                "-- The prompt you are generating is independent of the object which is questioner will try to guess. Focus on crafting a unique prompt for providing an AI model to be a questioner in the 20 Questions game.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all other prompts possbile]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )
        elif len(existing_and_candidate_prompts)!=0:

            instructions = (
                "Help me create a prompt which I can provide to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here is a list of existing prompts that have already been generated:\n"
                f"{existing_and_candidate_prompts_text}\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Do not combine, replicate, or produce anything similar to the above prompts.\n"
                "-- To ensure diversity in the prompts you generate, aim for lexical variety by minimizing the repetition of common words from above prompts.\n"
                "-- Your goal is to generate a completely new and unique prompt that is distinct from all of these and offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all the above prompts.\n"
                "-- The prompt you are generating is independent of the object which is questioner will try to guess. Focus on crafting a unique prompt for providing an AI model to be a questioner in the 20 Questions game.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all above prompts]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )
    if top_k_prompts is not None and bottom_k_prompts is not None:
        if len(existing_and_candidate_prompts) == 0:
            instructions = (
                "Help me create a prompt which I can provide to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here is a list of best and worst prompts:\n"
                f"Best prompts:\n{top_prompts_text}\n\nWorst prompts:\n{bottom_prompts_text}\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Do not combine, replicate, or produce anything similar to the above prompts.\n"
                "-- To ensure diversity in the prompts you generate, aim for lexical variety by minimizing the repetition of common words from above prompts.\n" 
                "-- Your goal is to generate a completely new and unique prompt that offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all the above prompts.\n"
                "-- The prompt you are generating is independent of the object which is questioner will try to guess. Focus on crafting a unique prompt for providing an AI model to be a questioner in the 20 Questions game.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all the above prompts]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "Ensure all fields are provided and have non-zero or non-empty values."
            )
        elif len(existing_and_candidate_prompts)!=0:
            instructions = (
                "Help me create a prompt which I can provide to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here is a list of best and worst prompts:\n"
                f"Best prompts:\n{top_prompts_text}\n\nWorst prompts:\n{bottom_prompts_text}\n\n"
                "Here is a list of existing prompts that have already been generated:\n"
                f"{existing_and_candidate_prompts_text}\n\n"
                "While generating prompts, keep the following things in mind:\n"
                "-- Do not combine, replicate, or produce anything similar to the above prompts.\n"
                "-- To ensure diversity in the prompts you generate, aim for lexical variety by minimizing the repetition of common words from above prompts.\n"
                "-- Your goal is to generate a completely new and unique prompt that is distinct from all of these and offers a fresh, innovative perspective.\n"
                "-- The prompt should demonstrate superior effectiveness and have the potential to outperform all the above prompts.\n"
                "-- The prompt you are generating is independent of the object which is questioner will try to guess. Focus on crafting a unique prompt for providing an AI model to be a questioner in the 20 Questions game.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for diversity": "[explain why this prompt is diverse]",\n'
                '  "Is this diverse": "[true/false]",\n'
                '  "Reason for best prompt": "[explain why this prompt is the best and hence can outperform all the above prompts]",\n'
                '  " Is this the best prompt": "[true/false]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

    return instructions

def check_prompts(no_of_prompts, file_name):
    """
    Parameters
    ----------
    no_of_prompts: int
        Number of prompts required for the 20 questions to be generated
    file_name: str
        The name of the file where the prompts are stored

    Returns
    --------
    bool
        True if the number of prompts stored in the prompts.csv are more than or equal to the number of prompts required, False otherwise
    """

    if not os.path.exists(file_name):
        return False
    
    with open(file_name, "r") as f:
        reader = csv.reader(f)
        data = list(reader)
        if len(data) >= no_of_prompts + 1:  # +1 for the header row
            return True
        else:
            return False

def get_last_prompt_id(file_name):
    """
    Parameters
    ----------
    file_name: str
        Name of the file to read the last prompt ID from

    Returns
    --------
    last_prompt_id: int
        The last used prompt ID
    """

    if not os.path.exists(file_name):
        return 0
    
    with open(file_name, "r") as f:
        reader = csv.reader(f)
        data = list(reader)
        if len(data) > 1:
            return int(data[-1][0])  # Return the last prompt ID
        else:
            return 0

def load_existing_prompts(file_name):
    """
    Parameters
    ----------
    file_name: str
        The name of the file where the prompts are stored

    Returns
    --------
    existing_prompts: set
        A set of existing prompts in the CSV file
    """
    existing_prompts = set()

    if os.path.exists(file_name):
        with open(file_name, "r") as f:
            reader = csv.reader(f)
            for row in reader:
                if row[0] != "Prompt ID":  # Skip the header row
                    existing_prompts.add(row[1].strip())

    return existing_prompts

def extract_best_prompt(json_response):
    """
    Extracts the best prompt and reason from the JSON response if it meets all criteria.
    """
    # Define the required keys
    required_keys = ["Reason for the prompt being the best", "Prompt Text"]

    # Check if all required keys are present and their values are non-zero or non-empty
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the prompt text and reason
        reason = json_response["Reason for the prompt being the best"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        prompt_text = json_response["Prompt Text"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        return prompt_text, reason
    else:
        # If any key is missing or has an empty/zero value, return None
        return None, None
    

# Finding the most diverse prompt from the 5 candidate prompts. The prompt has to be diverse from the existing prompts
def finding_best_prompt_from_candidates(existing_prompts, candidates, temp, max_tokens, top_k_prompts=None, bottom_k_prompts=None):

    if len(existing_prompts) != 0:
        existing_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(existing_prompts)])

    if top_k_prompts is None and bottom_k_prompts is None:
        if len(existing_prompts) != 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here are the 3 candidate prompts from which you need to select the best one:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "The existing prompts are:\n"
                f"{existing_prompts_text}\n\n"
                "Consider the following criteria while evaluating each response:\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "any of the other candidate prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
                
            )

        elif len(existing_prompts) == 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here are the 3 candidate prompts from which you need to select the best one:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "Consider the following criteria while evaluating each response:\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "any of the other candidate prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

    if top_k_prompts is not None and bottom_k_prompts is not None:

        top_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(top_k_prompts)])
        bottom_prompts_text = "\n".join([f'{i+1}. "Prompt: {prompt}"' for i, prompt in enumerate(bottom_k_prompts)])

        if len(existing_prompts) != 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here are the 3 candidate prompts from which you need to select the best one:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "The existing prompts are:\n"
                f"{existing_prompts_text}\n\n"
                "Additionally, here are some examples of the best and worst prompts:\n\n"
                "Best prompts:\n"
                f"{top_prompts_text}\n\n"
                "Worst prompts:\n"
                f"{bottom_prompts_text}\n\n"
                "Evaluate each response based on the following criteria:\n\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "all other candidate prompts as well as the best and worst prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

        elif len(existing_prompts) == 0:
            instruction_to_select_best_prompt = (
                "Help me select a prompt from the 5 candidate prompts that I can give to an AI model to be a questioner in the 20 Questions game.\n\n"
                "Here are the 3 candidate prompts:\n"
                f"1. \"{candidates[0]}\"\n"
                f"2. \"{candidates[1]}\"\n"
                f"3. \"{candidates[2]}\"\n\n"
                "Additionally, here are some examples of the best and worst prompts:\n\n"
                "Best prompts:\n"
                f"{top_prompts_text}\n\n"
                "Worst prompts:\n"
                f"{bottom_prompts_text}\n\n"
                "Evaluate each response based on the following criteria:\n"
                "- **Quality**: The prompt should demonstrate superior effectiveness and have the potential to outperform "
                "all other candidate prompts as well as the best and worst prompts.\n\n"
                "Make sure that the response is in JSON format with the following fields:\n"
                "{\n"
                '  "Reason for the prompt being the best": "[explain why this prompt is best among the candidates]",\n'
                '  "Prompt Text": "[write only your prompt here and dont start by Prompt:]"\n'
                "}\n\n"
                "Ensure all fields are provided and have non-zero or non-empty values."
            )

    candidate_prompts = [candidates[0], candidates[1], candidates[2]]

    times_runned =0
    while(times_runned<10):
        json_response = api_call_openai_json("gpt-4o-mini-2024-07-18", instruction_to_select_best_prompt, temp, max_tokens)
        best_prompt, reason = extract_best_prompt(json_response)
        if best_prompt is None or reason is None:
            print(json_response)
            print("Response did not contain the required keys, retrying...")
            times_runned += 1
            continue
        
         # Make a dictionary which has key as the prompt and value as a dictionary which has the reason and candidate prompts
        best_prompt_dict = {best_prompt: {"reason": reason, "candidates": candidates}}

        # Store this dict in a json file
        # Check if the json file already has data then update the data

        if os.path.exists(f"selected_prompt_dict.json"):
            with open(f"selected_prompt_dict.json", "r") as f:
                data = json.load(f)
                data.update(best_prompt_dict)
                with open(f"selected_prompt_dict.json", "w") as f:
                    json.dump(data, f)
        else:
            with open(f"selected_prompt_dict.json", "w") as f:
                json.dump(best_prompt_dict, f)

        return best_prompt, reason
        

def get_objects(no_of_objects, temp, max_tokens, file_path):
    """
    Parameters
    ----------
    no_of_objects: int
        Number of objects required for the 20 questions game

    Returns
    --------
    objects: list
        List of objects generated by LLM for the 20 questions game to be given to the answerer for the game
    """

    # object_set = set()
    # objects = []
    
    # last_object_id = get_last_object_id("objects.csv")

    # # Instruction for the objects to be generated    
    # instruction_for_objects = f"""I am a Large Language Model playing the 20-question game. In the game, your job is to help me win the game. Help me think of an object for my role as the answerer.

    # Give me {no_of_objects} unique objects that can help me win the game as an answerer. Print each item on a new line in the following format.

    # [
    #     Item1
    #     Item2
    #     Item3
    # ]
    
    # Here is an example of the format if asked to generate 3 objects -

    # [
    #     Table
    #     Chair
    #     Television
    # ]"""
    
    # # Check if the objects.csv file exists and read existing objects
    # if os.path.exists("objects.csv"):
    #     with open("objects.csv", mode='r') as file:
    #         csv_reader = csv.reader(file)
    #         for row in csv_reader:
    #             if row[0] != "Object ID":
    #                 objects.append(row[1])
    #         print(objects)

    # old_objects = set(objects)

    # while not check_objects(no_of_objects, file_path):
        
    #     if len(object_set) >= no_of_objects - len(old_objects):
    #         break

    #     response_text = api_call_openai("gpt-3.5-turbo-0125", instruction_for_objects, temp, max_tokens).split("[")[1].split("]")[0].split("\n")

    #     # Remove empty strings from the response_text and store them in the object_set
    #     response_text = [x.strip() for x in response_text if x.strip()]
        
    #     for obj in response_text:
    #         if obj not in old_objects:
    #             object_set.add(obj)
        
    # # Crop the length of the object set to the number of objects needed
    # object_set = list(object_set)[:no_of_objects - len(old_objects)]

    # with open("objects.csv", "a", newline="") as f:
    #     writer = csv.writer(f)
    #     if f.tell() == 0:
    #         writer.writerow(["Object ID", "Object"])
    #     for obj in object_set:
    #         last_object_id += 1
    #         writer.writerow([last_object_id, obj])
    
    # print("Objects generated successfully!")    
    
    # objects = list(old_objects | set(object_set))

    objects = pd.read_csv(file_path)["Object"].to_list()
    print(objects[:5])

    # Sample only the required number of objects
    objects = objects[:no_of_objects]

    return objects

# Function to check if the objects.csv file has equal or more than the number of objects required
def check_objects(no_of_objects):
    """
    Parameters
    ----------
    no_of_objects: int
        Number of objects required for the 20 questions to be generated

    Returns
    --------
    bool
        True if enough objects are generated, False otherwise
    """

    if not os.path.exists("objects.csv"):
        return False
    
    with open("objects.csv", "r") as f:
        reader = csv.reader(f)
        data = list(reader)
        if len(data) >= no_of_objects + 1:  # +1 for the header row
            return True
        else:
            return False

def get_last_object_id(file_name):
    """
    Parameters
    ----------
    file_name: str
        Name of the file to read the last object ID from

    Returns
    --------
    last_object_id: int
        The last used object ID
    """

    if not os.path.exists(file_name):
        return 0
    
    with open(file_name, "r") as f:
        reader = csv.reader(f)
        data = list(reader)
        if len(data) > 1:
            return int(data[-1][0])  # Return the last object ID
        else:
            return 0


#function to get the number of questions asked by the questioner to guess the object
def game(object, questioner_prompt, temp, max_tokens):

    """
    Parameters
    ----------
    object: str
        The object which the answerer is thinking of
    questioner_prompt: str
        The prompt given to the questioner to play the 20 questions game

    Returns
    --------
    results: List of all the questions asked by the questioner, the responses given by the answerer, the guesses made by the questioner to guess the object correctly, the number of questions asked by the questioner, and whether the questioner guessed the object correctly or not

    """
    
    question = ""
    guessed_correctly = False
    guesses = []
    times_runned = 0
    questions_asked = []
    responses = []
    no_of_questions = 0

    start_time = time.time()
    #play the game for 20 questions
    for _ in range(20):
        while True:
            if(times_runned>10):
                print("Failed to generate all objects")
                return [questioner_prompt, object, [], [], [], 20, False]
                
            
            question_json = api_call_openai_json("gpt-4o-mini-2024-07-18", prompt_to_questioner(questioner_prompt, no_of_questions, questions_asked, responses, guesses), temp, max_tokens)
            question = extract_question(question_json)
            if question is None:
                print("Question did not contain the required keys, retrying...")
                print(question_json)
                times_runned+=1
                continue
            else:
                questions_asked.append(question)
                break

        no_of_questions+=1

        while True:
            if(times_runned>10):
                print("Failed to answer the questions correclty")
                return [questioner_prompt, object, [], [], [], 20, False]
                

            response_json = api_call_openai_json("gpt-4o-mini-2024-07-18", prompt_to_answerer(object, question), temp, max_tokens)
            response = extract_answer(response_json)
            if response is None:
                print("Answer did not contain the required keys, retrying...")
                print(response_json)
                times_runned+=1
                continue
            else:
                responses.append(response)
                break
                    
        while True:
            if(times_runned>10):
                print("Failed to guess in correct format")
                return [questioner_prompt, object, [], [], [], 20, False]
                
            
            guess_json = api_call_openai_json("gpt-4o-mini-2024-07-18", prompt_to_guesser(no_of_questions, questions_asked, responses, guesses), temp, max_tokens)
            guess = extract_guess(guess_json)

            if guess is None:
                print("Guess did not contain the required keys, retrying...")
                print(guess_json)
                times_runned+=1
                continue
            else:
                guesses.append(guess)
                break
        #check if guessed object and the object are equal in lower case

        simple_ratio = fuzz.ratio(object.lower(), guesses[-1].lower())
        if simple_ratio >=85:
            guessed_correctly = True
            break
        # if guesses[-1].lower() == object.lower():
        #     guessed_correctly = True
        #     break

    end_time = time.time()
    print(f"Time taken to complete the game: {end_time - start_time} seconds")
    print("Game completed!")

    # make a list of all the questions asked by the questioner, the responses given by the answerer, the guesses made by the questioner, the number of questions asked by the questioner, and whether the questioner guessed the object correctly or not and then return the list named results
    results = [questioner_prompt, object, questions_asked, responses, guesses, no_of_questions, guessed_correctly]
    return results


def extract_question(json_response):
    """
    Extracts the question from the JSON response if it meets all criteria.
    """
    if json_response is None:
        return None
    # Define the required keys
    required_keys = ["question"]

    # Check if all required keys are present and their values are non-zero or non-empty
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the question
        question = json_response["question"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        return question
    else:
        # If any key is missing or has an empty/zero value, return None
        return None

def extract_answer(json_response):
    """
    Extracts the answer from the JSON response if it meets all criteria.
    """

    if json_response is None:
        return None
    # Define the required keys
    required_keys = ["answer"]

    # Check if all required keys are present and their values are non-zero or non-empty and make sure the answer is just a Yes or a no
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the answer
        answer = json_response["answer"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        if answer.lower() == "yes" or answer.lower() == "no":
            return answer
        else:
            return None
    else:
        # If any key is missing or has an empty/zero value, return None
        return None

def extract_guess(json_response):
    """
    Extracts the guess from the JSON response if it meets all criteria.
    """

    if json_response is None:
        return None
    # Define the required keys
    required_keys = ["guess"]

    # Check if all required keys are present and their values are non-zero or non-empty
    if all(key in json_response for key in required_keys) and all(json_response[key] for key in required_keys):
        # Extract the guess
        guess = json_response["guess"].strip().strip('"').strip("'").strip().strip("**").strip("*").strip().strip('"').strip()
        return guess
    else:
        # If any key is missing or has an empty/zero value, return None
        return None

def prompt_to_questioner(questioner_prompt, no_of_questions, questions_asked, responses, guesses):
    """
    Parameters
    ----------
    questioner_prompt: str
        The prompt given to the questioner to play the 20 questions game
    no_of_questions: int
        The number of questions asked by the questioner till now
    questions_asked: List
        List of all the questions asked by the questioner till now
    responses: List
        List of all the responses given by the answerer till now
    guesses: List
        List of all the incorrect guesses made by the questioner till now

    Returns
    --------
    to_questioner: str
        The prompt to be given to the questioner to ask the next question
    """

    to_questioner = (
        f"{questioner_prompt}\n\n"
        "You are the questioner in the 20-question game. Your job is to guess the object which the answerer is thinking of. "
        "Do this in at most twenty questions."
        "Ask only Dichotomous yes/no questions\n\n"
    )

    # Only add rounds if questions_asked is not empty
    if questions_asked:
        rounds = ""
        for i, (question, response, guess) in enumerate(zip(questions_asked, responses, guesses), start=1):
            rounds += (
                f"Round {i}:\n"
                f"Question: {question}\n"
                f"Answer: {response}\n"
                f"Guess: {guess}\n\n"
            )
        to_questioner += f"You have asked {no_of_questions} questions till now. Here are the details:\n\n{rounds}"

    to_questioner += (
        "Ask your next question to the answerer.\n\n"
        "Make sure that the response is in JSON format as follows:\n\n"
        '{\n  "question": "[your question here]"\n}'
    )

    return to_questioner

def prompt_to_answerer(object, question):

    """
    Parameters
    ----------
    object: str
        The object which the answerer is thinking of
    question: str
        The question asked by the questioner to the answerer
    
    Returns
    --------
    to_answerer: str
        The prompt to be given to the answerer to answer the question asked by the questioner
    """
    
    to_answerer = (
        f"You are the answerer in the 20-question game. The object you are thinking of is: {object}. "
        "Your job is to answer the questions asked by the questioner.\n\n"
        "Answer truthfully by just a Yes or a No keeping the object in mind.\n\n"
        f"The question asked to you by the questioner is: {question}\n\n"
        "Make sure that the response is in JSON format as follows:\n\n"
        '{\n  "answer": "[Yes/No]"\n}'
    )

    return to_answerer

def prompt_to_guesser(no_of_questions, questions_asked, responses, guesses):
    """
    Parameters
    ----------
    no_of_questions: int
        The number of questions asked by the questioner till now
    questions_asked: List
        List of all the questions asked by the questioner till now
    responses: List
        List of all the responses given by the answerer till now
    guesses: List
        List of all the incorrect guesses made by the question

    Returns
    --------
    to_guesser: str
        The prompt to be given to the guesser to make a guess of the object
    """
    
    rounds = ""
    for i, (question, response, guess) in enumerate(zip(questions_asked, responses, guesses), start=1):
        rounds += (
            f"Round {i}:\n"
            f"Question: {question}\n"
            f"Answer: {response}\n"
            f"Guess: {guess}\n\n"
        )

    to_guesser = (
        "You are the questioner in the 20-question game. Your job is to guess the object which the answerer is thinking of.\n\n"
        f"You have asked {no_of_questions} questions till now. Here are the details:\n\n"
        f"{rounds}"
        "Given the questions you have asked and the responses from the answerer, try to guess the object the answerer is thinking of.\n\n"
        "Make sure that the response is in JSON format as follows:\n\n"
        '{\n  "guess": "[Enter your guess here]"\n}'
        'Please make sure that your guess is just an object name and nothing else.'
    )

    return to_guesser


# # Function to record the results of the game, create the csv file as well
# def record_results(results):

#     """
#     Parameters
#     ----------
#     results: List
#         List of all the questions asked by the questioner, the responses given by the answerer, the guesses made by the questioner to guess the object correctly, the number of questions asked by the questioner, and whether the questioner guessed the object correctly or not
    
#     Returns
#     --------
#     None

#     """

#     with open("results.csv", "a", newline="") as f:
#         writer = csv.writer(f)
#         if f.tell() == 0:
#             writer.writerow(["Questioner Prompt", "Object", "Questions Asked", "Responses", "Guesses", "Number of Questions", "Guessed Correctly"])
#         writer.writerow(results)
        

# finding the ELO rating of the prompt after battle
def elo_rating(questioner_prompt1, questioner_prompt2, object, temp, max_tokens, game_over = False, result_1 = None, result_2 = None, custom_dict = None):
    
    """
    Parameters
    ----------
    questioner_prompt1: str
        The prompt given to the questioner to play the 20 questions game (First prompt)
    questioner_prompt2: str
        The prompt given to the questioner to play the 20 questions game (Second prompt)
    object: str
        The object which the answerer is thinking of

    Returns
    --------
    result_1: List
        List of all the questions asked by the questioner, the responses given by the answerer, the guesses made by the questioner to guess the object correctly, the number of questions asked by the questioner, and whether the questioner guessed the object correctly or not (First prompt)
    
    result_2: List
         List of all the questions asked by the questioner, the responses given by the answerer, the guesses made by the questioner to guess the object correctly, the number of questions asked by the questioner, and whether the questioner guessed the object correctly or not (Second prompt)

    winner: str
        The prompt which won the game
    
    """
    
    global elo_ratings_dict
    if game_over == False:
        # Get the results of the game1
        result_1 = game(object, questioner_prompt1, temp, max_tokens)
        # Get the results of the game2
        result_2 = game(object, questioner_prompt2, temp, max_tokens)

    # Get the number of questions asked in the game
    no_of_questions1 = result_1[5]
    guessed_correctly_1 = result_1[6]
   
    # Get the number of questions asked in the game
    no_of_questions2 = result_2[5]
    guessed_correctly_2 = result_2[6]
    # See who is the winner by comparing the number of questions asked as well as the guessed correctly, just set the winner and loser and if it is a draw then set the winner = "draw"
    if no_of_questions1 < no_of_questions2:
        winner = questioner_prompt1
        loser = questioner_prompt2
    elif no_of_questions1 > no_of_questions2:
        winner = questioner_prompt2
        loser = questioner_prompt1
    else:
        if guessed_correctly_1 > guessed_correctly_2:
            winner = questioner_prompt1
            loser = questioner_prompt2
        elif guessed_correctly_1 < guessed_correctly_2:
            winner = questioner_prompt2
            loser = questioner_prompt1
        else:
            winner = "draw"
            loser = "draw"

    if custom_dict == None:
    #If it is a draw, then calculate the ELO ratings of the prompts and print the updated ELO ratings
        if winner == "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = elo_ratings_dict[questioner_prompt1]
            loser_rating = elo_ratings_dict[questioner_prompt2]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (0.5 - expected_winner)
            loser_rating = loser_rating + 32 * (0.5 - expected_loser)
            # Update the ELO ratings in the dictionary
            elo_ratings_dict[questioner_prompt1] = winner_rating
            elo_ratings_dict[questioner_prompt2] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{questioner_prompt1}: {winner_rating}")
            print(f"{questioner_prompt2}: {loser_rating}")

        # Update the ELO ratings of the prompts if it is not a draw
        elif winner != "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = elo_ratings_dict[winner]
            loser_rating = elo_ratings_dict[loser]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (1 - expected_winner)
            loser_rating = loser_rating + 32 * (0 - expected_loser)
            # Update the ELO ratings in the dictionary
            elo_ratings_dict[winner] = winner_rating
            elo_ratings_dict[loser] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{winner}: {winner_rating}")
            print(f"{loser}: {loser_rating}")

        return result_1, result_2, winner
    
    if custom_dict != None:
           #If it is a draw, then calculate the ELO ratings of the prompts and print the updated ELO ratings
        if winner == "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = custom_dict[questioner_prompt1]
            loser_rating = custom_dict[questioner_prompt2]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (0.5 - expected_winner)
            loser_rating = loser_rating + 32 * (0.5 - expected_loser)
            # Update the ELO ratings in the dictionary
            custom_dict[questioner_prompt1] = winner_rating
            custom_dict[questioner_prompt2] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{questioner_prompt1}: {winner_rating}")
            print(f"{questioner_prompt2}: {loser_rating}")

        # Update the ELO ratings of the prompts if it is not a draw
        elif winner != "draw":
            # Find the ELO scores of the winner and loser
            winner_rating = custom_dict[winner]
            loser_rating = custom_dict[loser]
            # Calculate the expected score of the winner and loser
            expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
            expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
            # Update the ELO ratings of the winner and loser
            winner_rating = winner_rating + 32 * (1 - expected_winner)
            loser_rating = loser_rating + 32 * (0 - expected_loser)
            # Update the ELO ratings in the dictionary
            custom_dict[winner] = winner_rating
            custom_dict[loser] = loser_rating
            # Print the updated ELO ratings
            print("Updated ELO Ratings:")
            print(f"{winner}: {winner_rating}")
            print(f"{loser}: {loser_rating}")

        return result_1, result_2, winner


# Function to check if the i'th league is already over or not to avoid playing the league again by checking if the league number is present in the dictionary of leage_results.json file
def check_league(league_no):
    
        """
        Parameters
        ----------
        league_no: int
            The league number to be checked if it is already over or not
    
        Returns
        --------
        Bool: True if the league is already over, False otherwise
    
        """

        if os.path.exists('league_results.json') == False:
            return False
        
        # Load the dictionary from the JSON file
        with open('league_results.json', 'r') as json_file:
            data = json.load(json_file)
        
        # Check if the league number is present in the dictionary
        if str(league_no) in data:
            return True
        else:
            return False

# Function which takes all the prompts and objects as input and plays a round robin league and updates the elo ratings of the prompts and returns the top 10 and bottom 10 prompts after the league
def league(prompts, objects, k, league_no, temp, max_tokens, human = False, human_file = None):

    """
    Parameters
    ----------
    prompts: List
        List of all the prompts to be used in the league
    objects: List
        List of all the objects to be used in the league
    k: int
        Number of top and bottom prompts to be returned
    league_no: int
        The league number for which the league is to be
    human: bool
        True if the prompts playing the league are given by humans, False if the prompts are generated by LLM
    human_file: str
        The name of the file where the human prompts are stored
    final: bool
        True if it is the final league, False otherwise

        
    Returns
    --------
    top_10_prompts: List
        List of the top 10 prompts with the highest ELO ratings after the league
    bottom_10_prompts: List
        List of the bottom 10 prompts with the lowest ELO ratings after the league
    """

    global elo_ratings_dict
    global league_dict
    global top_bottom_prompts_dict
    global human_prompts_used_dict
    global origin_league_prompts_dict

    for prompt in prompts:
        origin_league_prompts_dict[prompt] = league_no


    #Check if the league is already palyed or not
    if check_league(league_no):

        print(f"League {league_no} is already over!")
        # Get the top and bottom k prompts along with their final elo scores from the league_results.json file for that league_no and also put it in the top_bottom_prompts_dict
        with open('league_results.json', 'r') as json_file:
            data = json.load(json_file)
        
        # Collect all prompts with their final ELO scores
        all_prompts = {}
        for battle_no, battle_dict in data[str(league_no)].items():
            prompt_1 = battle_dict['prompt_1']
            prompt_2 = battle_dict['prompt_2']
            all_prompts[prompt_1['questioner_prompt']] = prompt_1['final_elo']
            all_prompts[prompt_2['questioner_prompt']] = prompt_2['final_elo']


        # Store all the prompts in the human_prompts_used_dict if human prompts are used
        if human == True and human_file != None:
            human_prompts_used_dict[league_no] = all_prompts.keys()

        # Store all the elo scores in the elo_ratings_dict with the key has prompt and value as elo score
        for prompt, elo in all_prompts.items():
            elo_ratings_dict[prompt] = elo

        # Sort prompts by ELO score
        sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1], reverse=True)

        # Get the top k and bottom k prompts
        top_k_prompts = sorted_prompts[:k]
        bottom_k_prompts = sorted_prompts[-k:]

        # Save the top and bottom k prompts in the dictionary for the league and also rank them by 1,2,3..,k
        top_bottom_prompts_dict[league_no] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
        }

        # Dump the top_bottom_prompts_dict in the json file
        with open('top_bottom_prompts_dict.json', 'w') as json_file:
            json.dump(top_bottom_prompts_dict, json_file, indent=4)

        return top_k_prompts, bottom_k_prompts
    
    temp_league_dict = {}
    temp_league_dict[league_no] = {}

    # Initialize elo ratings of all prompts to 1200 until and unless it is the final league
    
    for prompt in prompts:
        elo_ratings_dict[prompt] = 1200

    battle_no = 0
    round_robin_list = [(i, j) for i in range(len(prompts)) for j in range(i + 1, len(prompts))]
    random.shuffle(round_robin_list)

    # To store results for ELO updates after parallel processing
    results_list = [None] * len(round_robin_list)

    start_time = time.time()

    # Use ThreadPoolExecutor to run games in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
        future_to_battle = {}

        for index, (i, j) in enumerate(round_robin_list):
            # Select the same object for both prompts in this battle
            object = random.choice(objects)
            
            # Submit two tasks: one for each prompt in the battle with the same object
            future_1 = executor.submit(game, object, prompts[i], temp, max_tokens)
            future_2 = executor.submit(game, object, prompts[j], temp, max_tokens)
            
            # Store both futures with their respective indices
            future_to_battle[future_1] = (index, i, j, 'first')
            future_to_battle[future_2] = (index, i, j, 'second')

        # Process results as they complete
        results_dict = {}  # Dictionary to hold results temporarily
        for future in concurrent.futures.as_completed(future_to_battle):
            index, i, j, which = future_to_battle[future]
            result = future.result()
            if which == 'first':
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][0] = result
                else:
                    results_dict[(index, i, j)] = [result, None]
            else:
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][1] = result  # Store result_2
                else:
                    results_dict[(index, i, j)] = [None, result]
    
        # Convert dictionary to list to maintain order
        results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

    # Process results in order of the original round robin list
    for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
        # Use the same object that was chosen for the battle
        print(result_1)
        object = result_1[1]  # Object is the same for both prompts
        # Get initial ELO ratings
        initial_elo_rating_1 = elo_ratings_dict[prompts[i]]
        initial_elo_rating_2 = elo_ratings_dict[prompts[j]]

        # Update ELO ratings using elo_rating function
        _, _, winner = elo_rating(prompts[i], prompts[j], object, temp, max_tokens, game_over=True, result_1=result_1, result_2=result_2)
        
        # Get final ELO ratings
        final_elo_rating1 = elo_ratings_dict[prompts[i]]
        final_elo_rating2 = elo_ratings_dict[prompts[j]]

        # Increment battle number and prepare battle dictionary
        battle_no += 1
        battle_dict = {
            'object_name': object,
            'prompt_1': {
                'questioner_prompt': result_1[0] if result_1 else prompts[i],
                'object': result_1[1] if result_1 else object,
                'questions_asked': result_1[2] if result_1 else [],
                'responses': result_1[3] if result_1 else [],
                'guesses': result_1[4] if result_1 else [],
                'no_of_questions': result_1[5] if result_1 else 0,
                'guessed_correctly': result_1[6] if result_1 else False,
                'initial_elo': initial_elo_rating_1,
                'final_elo': final_elo_rating1
            },
            'prompt_2': {
                'questioner_prompt': result_2[0] if result_2 else prompts[j],
                'object': result_2[1] if result_2 else object,
                'questions_asked': result_2[2] if result_2 else [],
                'responses': result_2[3] if result_2 else [],
                'guesses': result_2[4] if result_2 else [],
                'no_of_questions': result_2[5] if result_2 else 0,
                'guessed_correctly': result_2[6] if result_2 else False,
                'initial_elo': initial_elo_rating_2,
                'final_elo': final_elo_rating2
            },
            'winner': winner
        }

        temp_league_dict[league_no][battle_no] = battle_dict

    end_time = time.time()
    print(f"Time taken to complete the league: {end_time - start_time} seconds")

    
    # Store the prompts used in the human_prompts_used_dict if human prompts are used
    if human == True and human_file != None:
        human_prompts_used_dict[league_no] = prompts


    temp_elo_dict={}
    for i in range(len(prompts)):
        temp_elo_dict[prompts[i]]=elo_ratings_dict[prompts[i]]

    sorted_prompts = sorted(temp_elo_dict.items(), key=lambda x: x[1], reverse=True)

    # Get the top k and bottom k prompts
    top_k_prompts = sorted_prompts[:k]
    bottom_k_prompts = sorted_prompts[-k:]
    
    # Check if the JSON file exists
    file_path = 'league_results.json'
    if os.path.exists(file_path):
        # If the file exists, load the existing data
        with open(file_path, 'r') as json_file:
            data = json.load(json_file)
    else:
        # If the file does not exist, start with an empty dictionary
        data = {}

    # Update the data with the current league_dict
    data.update(temp_league_dict)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)
    
    # Save the top and bottom k prompts in the dictionary for the league and also rank them by 1,2,3..,k
    top_bottom_prompts_dict[league_no] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
    }


    # Dump the top_bottom_prompts_dict in the json file
    with open('top_bottom_prompts_dict.json', 'w') as json_file:
        json.dump(top_bottom_prompts_dict, json_file, indent=4)

    # # Load the dictionary from the pickle file
    # with open('league_results.pkl', 'rb') as pickle_file:
    #     league_dict = pickle.load(pickle_file)

    # return top and bottom k prompts


    return top_k_prompts, bottom_k_prompts


# Function which takes all the prompts and objects as input and plays a round robin league and updates the elo ratings of the prompts and returns the top 10 and bottom 10 prompts after the league
def final_league(prompts, objects, k, league_no, temp, max_tokens):
    
    global top_bottom_prompts_dict_across_league
    global origin_league_prompts_dict

    final_elo_dict = {}

    if os.path.exists(f'final_league_results{league_no}.json'):
        print(f"Final League {league_no} is already over!")
        # Get the top and bottom k prompts for pro and con side along with their final elo scores from the final_league_results{league_no}.json file for that league_no
        with open(f'final_league_results{league_no}.json', 'r') as json_file:
            data = json.load(json_file)
        
        # Collect all prompts with their final ELO scores
        all_prompts = {}
        for battle_no, battle_dict in data[str(league_no)].items():
            prompt_1 = battle_dict['prompt_1']
            prompt_2 = battle_dict['prompt_2']
            all_prompts[prompt_1['questioner_prompt']] = prompt_1['final_elo']
            all_prompts[prompt_2['questioner_prompt']] = prompt_2['final_elo']

        # Store all the elo scores in the elo_ratings_dict with the key has prompt and value as elo score
        for prompt, elo in all_prompts.items():
            final_elo_dict[prompt] = elo

        # Sort prompts by ELO score
        sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1], reverse=True)

        # Get the top k and bottom k prompts
        top_k_prompts = sorted_prompts[:k]
        bottom_k_prompts = sorted_prompts[-k:]

        # Store the top_k_prompts and bottom_k_prompts in top_bottom_prompts_dict_across_league
        top_bottom_prompts_dict_across_league[league_no] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
        }

        with open('top_bottom_prompts_dict_across_league.json', 'w') as json_file:
            json.dump(top_bottom_prompts_dict_across_league, json_file, indent=4)

        return top_k_prompts, bottom_k_prompts, final_elo_dict
    
    temp_league_dict = {}
    temp_league_dict[league_no] = {}

    
    for prompt in prompts:
        final_elo_dict[prompt] = 1200

    battle_no = 0
    round_robin_list = [(i, j) for i in range(len(prompts)) for j in range(i + 1, len(prompts))]
    random.shuffle(round_robin_list)

    # To store results for ELO updates after parallel processing
    results_list = [None] * len(round_robin_list)

    start_time = time.time()

    # Use ThreadPoolExecutor to run games in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
        future_to_battle = {}

        for index, (i, j) in enumerate(round_robin_list):
            # Select the same object for both prompts in this battle
            object = random.choice(objects)
            
            # Submit two tasks: one for each prompt in the battle with the same object
            future_1 = executor.submit(game, object, prompts[i], temp, max_tokens)
            future_2 = executor.submit(game, object, prompts[j], temp, max_tokens)
            
            # Store both futures with their respective indices
            future_to_battle[future_1] = (index, i, j, 'first')
            future_to_battle[future_2] = (index, i, j, 'second')

        # Process results as they complete
        results_dict = {}  # Dictionary to hold results temporarily
        for future in concurrent.futures.as_completed(future_to_battle):
            index, i, j, which = future_to_battle[future]
            result = future.result()
            if which == 'first':
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][0] = result
                else:
                    results_dict[(index, i, j)] = [result, None]
            else:
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][1] = result  # Store result_2
                else:
                    results_dict[(index, i, j)] = [None, result]
    
        # Convert dictionary to list to maintain order
        results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

    # Process results in order of the original round robin list
    for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
        # Use the same object that was chosen for the battle
        object = result_1[1]  # Object is the same for both prompts
        # Get initial ELO ratings
        initial_elo_rating_1 = final_elo_dict[prompts[i]]
        initial_elo_rating_2 = final_elo_dict[prompts[j]]

        # Update ELO ratings using elo_rating function
        _, _, winner = elo_rating(prompts[i], prompts[j], object, temp, max_tokens, game_over=True, result_1=result_1, result_2=result_2, custom_dict=final_elo_dict)
        
        # Get final ELO ratings
        final_elo_rating1 = final_elo_dict[prompts[i]]
        final_elo_rating2 = final_elo_dict[prompts[j]]

        # Increment battle number and prepare battle dictionary
        battle_no += 1
        battle_dict = {
            'object_name': object,
            'prompt_1': {
                'questioner_prompt': result_1[0] if result_1 else prompts[i],
                'object': result_1[1] if result_1 else object,
                'questions_asked': result_1[2] if result_1 else [],
                'responses': result_1[3] if result_1 else [],
                'guesses': result_1[4] if result_1 else [],
                'no_of_questions': result_1[5] if result_1 else 0,
                'guessed_correctly': result_1[6] if result_1 else False,
                'initial_elo': initial_elo_rating_1,
                'final_elo': final_elo_rating1
            },
            'prompt_2': {
                'questioner_prompt': result_2[0] if result_2 else prompts[j],
                'object': result_2[1] if result_2 else object,
                'questions_asked': result_2[2] if result_2 else [],
                'responses': result_2[3] if result_2 else [],
                'guesses': result_2[4] if result_2 else [],
                'no_of_questions': result_2[5] if result_2 else 0,
                'guessed_correctly': result_2[6] if result_2 else False,
                'initial_elo': initial_elo_rating_2,
                'final_elo': final_elo_rating2
            },
            'winner': winner
        }

        temp_league_dict[league_no][battle_no] = battle_dict

    end_time = time.time()
    print(f"Time taken to complete the league: {end_time - start_time} seconds")


    temp_elo_dict={}
    for i in range(len(prompts)):
        temp_elo_dict[prompts[i]]=final_elo_dict[prompts[i]]

    sorted_prompts = sorted(temp_elo_dict.items(), key=lambda x: x[1], reverse=True)

    # Get the top k and bottom k prompts
    top_k_prompts = sorted_prompts[:k]
    bottom_k_prompts = sorted_prompts[-k:]
    
    # Check if the JSON file exists
    file_path = f'final_league_results{league_no}.json'
    
    data = {}

    # Update the data with the current league_dict
    data.update(temp_league_dict)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)

    # Store the top_k_prompts and bottom_k_prompts in top_bottom_prompts_dict_across_league
    top_bottom_prompts_dict_across_league[league_no] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
    }

    # Dump the top_bottom_prompts_dict_across_league in the json file
    with open('top_bottom_prompts_dict_across_league.json', 'w') as json_file:
        json.dump(top_bottom_prompts_dict_across_league, json_file, indent=4)


    return top_k_prompts, bottom_k_prompts, final_elo_dict


def compare_elo_average(top_prompts_league, final_elo_dict, league_no, k, consecutive_fails):

    elo_average_league = 0
    for prompt in top_prompts_league:
        elo_average_league += final_elo_dict[prompt]
    elo_average_league = elo_average_league / len(top_prompts_league)

    # Get the top k prompts from the top_bottom_prompts_dict_across_league.json for league_no -1
    with open('top_bottom_prompts_dict_across_league.json', 'r') as json_file:
        data = json.load(json_file)
    
    top_prompts_final = data[str(league_no-1)]['top_k_prompts']
    top_prompts_final = [top_prompts_final[str(i)]['prompt'] for i in range(1, k+1)]

    elo_average_final = 0
    for prompt in top_prompts_final:
        elo_average_final += final_elo_dict[prompt]
    elo_average_final = elo_average_final / len(top_prompts_final)


    if (elo_average_final > elo_average_league):
        consecutive_fails +=1
        print("consecutive_fails: ", consecutive_fails)
        
    else:
        consecutive_fails = 0
        print("consecutive_fails: ", consecutive_fails)

    return consecutive_fails

def proof_of_concept(league1, league2, objects, k , temp, max_tokens):

    global top_bottom_prompts_dict_across_league
    global top_bottom_prompts_dict
    global origin_league_prompts_dict
    global top_bottom_prompts_dict_poc

    league_no = 1
    

    poc_elo_dict = {}

    if league1 !=1:
        # Get pro_prompts1 by getting top k prompts from the league1 using top_bottom_prompts_dict_across_league
        top_prompts_league1 = top_bottom_prompts_dict_across_league[league1]['top_k_prompts']
        prompts1 = [prompt['prompt'] for prompt in top_prompts_league1.values()]

        # Get pro_prompts2 by getting top k prompts from the league2 using top_bottom_prompts_dict_across_league
        top_prompts_league2 = top_bottom_prompts_dict_across_league[league2]['top_k_prompts']
        prompts2 = [prompt['prompt'] for prompt in top_prompts_league2.values()]

    if league1 == 1:

        # Get the pro_prompts1 by getting top k prompts from the league1 using top_bottom_prompts_dict
        top_prompts_league1 = top_bottom_prompts_dict[league1]['top_k_prompts']
        prompts1 = [prompt['prompt'] for prompt in top_prompts_league1.values()]

        # Get the pro_prompts2 by getting top k prompts from the league2 using top_bottom_prompts_dict_across_league
        top_prompts_league2 = top_bottom_prompts_dict_across_league[league2]['top_k_prompts']
        prompts2 = [prompt['prompt'] for prompt in top_prompts_league2.values()]
    
    # Merge the pro_prompts1 and pro_prompts2 to get the pro_prompts
    prompts = prompts1 + prompts2


    #Check if the file exists
    if os.path.exists(f'proof_of_concept_{league1}_{league2}.json'):
        print(f"Proof of Concept {league1} vs {league2} is already over!")
        # Get the top and bottom k prompts for pro and con side along with their final elo scores from the proof_of_concept_{league1}_{league2}.json' file for that league_no
        with open(f'proof_of_concept_{league1}_{league2}.json', 'r') as json_file:
            data = json.load(json_file)
        # Collect all prompts with their final ELO scores
        all_prompts = {}
        for battle_no, battle_dict in data[str(league_no)].items():
            prompt_1 = battle_dict['prompt_1']
            prompt_2 = battle_dict['prompt_2']
            all_prompts[prompt_1['questioner_prompt']] = prompt_1['final_elo']
            all_prompts[prompt_2['questioner_prompt']] = prompt_2['final_elo']

        # Store all the elo scores in the elo_ratings_dict with the key has prompt and value as elo score
        for prompt, elo in all_prompts.items():
            poc_elo_dict[prompt] = elo

        # Sort prompts by ELO score
        sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1], reverse=True)

        # Get the top k and bottom k prompts
        top_k_prompts = sorted_prompts[:k]
        bottom_k_prompts = sorted_prompts[-k:]

        # Store the top and bottom k prompts in top_bottom_prompts_dict_poc along with origin
        top_bottom_prompts_dict_poc[f"{league1} vs {league2}"] = {
        'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
        'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
        }

        # Dump the top_bottom_prompts_dict_poc in the json file
        with open('top_bottom_prompts_dict_poc.json', 'w') as json_file:
            json.dump(top_bottom_prompts_dict_poc, json_file, indent=4)

        compare_poc_elo_average(poc_elo_dict, prompts1, prompts2, league1, league2)

        return
    
    temp_league_dict = {}
    temp_league_dict[league_no] = {}

    
    for prompt in prompts:
        poc_elo_dict[prompt] = 1200

    battle_no = 0
    round_robin_list = [(i, j) for i in range(len(prompts)) for j in range(i + 1, len(prompts))]
    random.shuffle(round_robin_list)

    # To store results for ELO updates after parallel processing
    results_list = [None] * len(round_robin_list)

    start_time = time.time()

    # Use ThreadPoolExecutor to run games in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
        future_to_battle = {}

        for index, (i, j) in enumerate(round_robin_list):
            # Select the same object for both prompts in this battle
            object = random.choice(objects)
            
            # Submit two tasks: one for each prompt in the battle with the same object
            future_1 = executor.submit(game, object, prompts[i], temp, max_tokens)
            future_2 = executor.submit(game, object, prompts[j], temp, max_tokens)
            
            # Store both futures with their respective indices
            future_to_battle[future_1] = (index, i, j, 'first')
            future_to_battle[future_2] = (index, i, j, 'second')

        # Process results as they complete
        results_dict = {}  # Dictionary to hold results temporarily
        for future in concurrent.futures.as_completed(future_to_battle):
            index, i, j, which = future_to_battle[future]
            result = future.result()
            if which == 'first':
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][0] = result
                else:
                    results_dict[(index, i, j)] = [result, None]
            else:
                if (index, i, j) in results_dict:
                    results_dict[(index, i, j)][1] = result  # Store result_2
                else:
                    results_dict[(index, i, j)] = [None, result]
    
        # Convert dictionary to list to maintain order
        results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

    # Process results in order of the original round robin list
    for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
        # Use the same object that was chosen for the battle
        object = result_1[1]  # Object is the same for both prompts
        # Get initial ELO ratings
        initial_elo_rating_1 = poc_elo_dict[prompts[i]]
        initial_elo_rating_2 = poc_elo_dict[prompts[j]]

        # Update ELO ratings using elo_rating function
        _, _, winner = elo_rating(prompts[i], prompts[j], object, temp, max_tokens, game_over=True, result_1=result_1, result_2=result_2, custom_dict=poc_elo_dict)
        
        # Get final ELO ratings
        final_elo_rating1 = poc_elo_dict[prompts[i]]
        final_elo_rating2 = poc_elo_dict[prompts[j]]

        # Increment battle number and prepare battle dictionary
        battle_no += 1
        battle_dict = {
            'object_name': object,
            'prompt_1': {
                'questioner_prompt': result_1[0] if result_1 else prompts[i],
                'object': result_1[1] if result_1 else object,
                'questions_asked': result_1[2] if result_1 else [],
                'responses': result_1[3] if result_1 else [],
                'guesses': result_1[4] if result_1 else [],
                'no_of_questions': result_1[5] if result_1 else 0,
                'guessed_correctly': result_1[6] if result_1 else False,
                'initial_elo': initial_elo_rating_1,
                'final_elo': final_elo_rating1
            },
            'prompt_2': {
                'questioner_prompt': result_2[0] if result_2 else prompts[j],
                'object': result_2[1] if result_2 else object,
                'questions_asked': result_2[2] if result_2 else [],
                'responses': result_2[3] if result_2 else [],
                'guesses': result_2[4] if result_2 else [],
                'no_of_questions': result_2[5] if result_2 else 0,
                'guessed_correctly': result_2[6] if result_2 else False,
                'initial_elo': initial_elo_rating_2,
                'final_elo': final_elo_rating2
            },
            'winner': winner
        }

        temp_league_dict[league_no][battle_no] = battle_dict

    end_time = time.time()
    print(f"Time taken to complete the league: {end_time - start_time} seconds")


    temp_elo_dict={}
    for i in range(len(prompts)):
        temp_elo_dict[prompts[i]]=poc_elo_dict[prompts[i]]

    sorted_prompts = sorted(temp_elo_dict.items(), key=lambda x: x[1], reverse=True)

    # Get the top k and bottom k prompts
    top_k_prompts = sorted_prompts[:k]
    bottom_k_prompts = sorted_prompts[-k:]
    
    # Check if the JSON file exists
    file_path = f'proof_of_concept_{league1}_{league2}.json'
    
    data = {}

    # Update the data with the current league_dict
    data.update(temp_league_dict)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)

    # Store the top and bottom k prompts in top_bottom_prompts_dict_poc along with origin
    top_bottom_prompts_dict_poc[f"{league1} vs {league2}"] = {
    'top_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(top_k_prompts)},
    'bottom_k_prompts': {i+1: {'prompt': prompt, 'elo': elo, 'origin': origin_league_prompts_dict[prompt]} for i, (prompt, elo) in enumerate(bottom_k_prompts)}
    }

    # Dump the top_bottom_prompts_dict_poc in the json file
    with open('top_bottom_prompts_dict_poc.json', 'w') as json_file:
        json.dump(top_bottom_prompts_dict_poc, json_file, indent=4)


    compare_poc_elo_average(poc_elo_dict, prompts1, prompts2, league1, league2)

    return


def compare_poc_elo_average(poc_elo_dict, prompts1, prompts2, league1, league2):
    # Get the average of prompts1, prompts2,  using the poc_elo_dict
    elo_average_prompts1 = 0
    for prompt in prompts1:
        elo_average_prompts1 += poc_elo_dict[prompt]
    elo_average_prompts1 = elo_average_prompts1/len(prompts1)
    print("elo_average_prompts1: ", elo_average_prompts1)
    elo_average_prompts2 = 0
    for prompt in prompts2:
        elo_average_prompts2 += poc_elo_dict[prompt]
    elo_average_prompts2 = elo_average_prompts2/len(prompts2)
    print("elo_average_prompts2: ", elo_average_prompts2)

    if (elo_average_prompts1 > elo_average_prompts2):
        print(f"Proof of Concept {league1} vs {league2} Failed!")
    else:
        print(f"Proof of Concept {league1} vs {league2} Passed!")


#Function to play the tournament. In this, first we play the initial league on the prompts generated by the get_prompts() function and then we get the top and bottom k prompts. Then we generate new prompts using the get_new_prompts() function and play the league on these new prompts. We repeat this process for n-1 number of leagues. We give the top and bottom k prompts of current league as input to the get_new_prompts() function to generate new prompts for the next league. Then we play the n'th league which is the final league which takes the top k prompts and bottom k from each of the n-1 leagues and conducts a final league on these prompts. The top k prompts from this final league are the top k prompts of the tournament.
def tournament(no_of_objects, no_of_prompts_start, no_of_prompts_between, k, n, temp, max_tokens, current_directory):
    
        """
        Parameters
        ----------
        no_of_objects: int
            Number of objects required for the 20 questions game
        no_of_prompts_start: int
            Number of prompts required for the 20 questions game for the first league
        no_of_prompts_between: int
            Number of prompts required for the 20 questions game for the leagues in between
        k: int
            Number of top and bottom prompts to be returned
        n: int
            Number of leagues to be played
        
        Returns
        --------
        None
        """

        global top_bottom_prompts_dict
        global elo_ratings_dict
        global current_league

        file_path = os.path.join(current_directory, 'humans.csv')
        file_path_2 = os.path.join(current_directory, 'objects.csv')

        objects = get_objects(no_of_objects, temp, max_tokens, file_path_2)
        consecutive_fails = 0
        # Play all the leagues
        while(consecutive_fails<3):
            print(f"League {current_league}:\n")
            # Play the first league
            if (current_league ==1):

                prompts = get_prompts(no_of_prompts_start, current_league, temp, max_tokens, None, None, False, None)
    
                if prompts is None:
                    print("Invalid Input")
                    return
                
                top_k_prompts_league1, bottom_k_prompts_league1 = league(prompts, objects, k, current_league, temp, max_tokens, False, None)
                # Get only the prompts from the top_k_prompts_league1 and bottom_k_prompts_league1
                top_k_prompts_league1 = [prompt for prompt, elo in top_k_prompts_league1]
                bottom_k_prompts_league1 = [prompt for prompt, elo in bottom_k_prompts_league1]
                current_league+=1

            elif (current_league !=1):

                if(current_league==6):
                    break

                if (current_league ==2):
                # Generate new prompts for the next league using the top k and bottom k prompts from the previous league
                    prompts = get_prompts(no_of_prompts_between,current_league, temp, max_tokens,top_k_prompts_league1, bottom_k_prompts_league1 ,False, None)

                    if prompts is None:
                        print("Invalid Input")
                        return

                # Play the league on the new prompts
                    top_prompts_league, bottom_prompts_league = league(prompts, objects, k, current_league, temp, max_tokens, False, None)
                    
                    # Get only the prompts from the top_prompts_league, bottom_prompts_league
                    top_prompts_league = [prompt for prompt, elo in top_prompts_league]
                    bottom_prompts_league = [prompt for prompt, elo in bottom_prompts_league]
                    

                    # Merge the top prompts of league 1 and 2 into a new list
                    merged_top_prompts = top_k_prompts_league1 + top_prompts_league
                    merged_bottom_prompts = bottom_k_prompts_league1 + bottom_prompts_league
                    

                # Play the final league using the merged top prompts from league 1 and 2
                    top_prompts_final, bottom_prompts_final, final_elo_dict = final_league(merged_top_prompts, objects, k, current_league, temp, max_tokens)

                    # Get only the prompts from the top_prompts_final, bottom_prompts_final
                    top_prompts_final = [prompt for prompt, elo in top_prompts_final]
                    bottom_prompts_final = [prompt for prompt, elo in bottom_prompts_final]

                    current_league+=1
                
                elif(current_league>2):

                    # Merge the top pro prompts of final
                    prompts = get_prompts(no_of_prompts_between,current_league, temp, max_tokens,top_prompts_final, bottom_prompts_league ,False, None)
                   

                    if prompts is None:
                        print("Invalid Input")
                        return
                    
                    top_prompts_league, bottom_prompts_league = league(prompts, objects, k, current_league, temp, max_tokens, False, None)

                    # Get only the prompts from the top_prompts_league, bottom_prompts_league
                    top_prompts_league = [prompt for prompt, elo in top_prompts_league]
                    bottom_prompts_league = [prompt for prompt, elo in bottom_prompts_league]
                
                    # Merge the top and bottom prompts of league and final
                    merged_top_prompts = top_prompts_league + top_prompts_final
                    merged_bottom_prompts = bottom_prompts_league + bottom_prompts_final
                    
                    # Play the final league using the merged top pro and con prompts from league and final
                    top_prompts_final, bottom_prompts_final, final_elo_dict = final_league(merged_top_prompts, objects, k, current_league, temp, max_tokens)

                    # Get only the prompts from the top_prompts_final, bottom_prompts_final
                    top_prompts_final = [prompt for prompt, elo in top_prompts_final]
                    bottom_prompts_final = [prompt for prompt, elo in bottom_prompts_final]

                    consecutive_fails = compare_elo_average(top_prompts_league, final_elo_dict, current_league, k, consecutive_fails)

                    if (consecutive_fails>=3):
                        break
                
                    current_league+=1
        
                
        last_league_no = 2
        # Find last league number using prompts{last_league_no}.json file
        while(f'prompts_{last_league_no}.csv' in os.listdir()):
            last_league_no+=1
        last_league_no-=1
        
        proof_of_concept(1, last_league_no, objects, k , temp, max_tokens)
        proof_of_concept(2, last_league_no, objects, k , temp, max_tokens)
        

# # Function to run the final league for all the values less than to n by extracting top k prompts from previous leagues and playing the final league
# def run_final_leagues(no_of_objects, n, k, temp, max_tokens, file_path):

#     # Read the league_results.json file
#     with open('league_results.json', 'r') as json_file:
#         data = json.load(json_file)
        
#     objects  = get_objects(no_of_objects, temp, max_tokens, file_path)
#     # Play all the final leagues from 3 to n-1
#     for z in range(3, n):
#         specific_elo_ratings_dict = {}
#         league_dictionary = {}
#         top_prompts = []
#         league_number = z
#         # Check if the league is already played or not
#         if (f'final_league_results{z}.json') in os.listdir():
#             print(f"Final League {z} is already over!")
#             continue
#         league_dictionary[league_number] = {}
#         battle_number = 0

#         # Collect all the questoner prompts from the previous leagues of i using league_resutls.json file along with their elo scores
#         all_prompts = {}

#         for league_no in range(1, z):
#             all_prompts[league_no] = {}
#             for battle_no, battle_dict in data[str(league_no)].items():
#                 prompt_1 = battle_dict['prompt_1']['questioner_prompt']
#                 prompt_2 = battle_dict['prompt_2']['questioner_prompt']
#                 prompt_1_elo = battle_dict['prompt_1']['final_elo']
#                 prompt_2_elo = battle_dict['prompt_2']['final_elo']
#                 all_prompts[prompt_1] = prompt_1_elo
#                 all_prompts[prompt_2] = prompt_2_elo
            

#             sorted_prompts = sorted(all_prompts[league_no].items(), key=lambda x: x[1], reverse=True)
#             top_k_prompts = sorted_prompts[:k]
#             for prompt, elo in top_k_prompts:
#                 specific_elo_ratings_dict[prompt] = elo
#                 top_prompts.append(prompt)

        
#         # Generate the round robin list with pair (i, j) where i is the index of one prompt and j is the index of another prompt
#         round_robin_list = [(i, j) for i in range(len(top_prompts)) for j in range(i + 1, len(top_prompts))]
#         random.shuffle(round_robin_list)

#         # To store results for ELO updates after parallel processing
#         results_list = [None] * len(round_robin_list)

#         start_time = time.time()

#         # Use ThreadPoolExecutor to run games in parallel
#         with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
#             future_to_battle = {}

#             for index, (i, j) in enumerate(round_robin_list):
#                 # Select the same object for both prompts in this battle
#                 object = random.choice(objects)
                
#                 # Submit two tasks: one for each prompt in the battle with the same object
#                 future_1 = executor.submit(game, object, top_prompts[i], temp, max_tokens)
#                 future_2 = executor.submit(game, object, top_prompts[j], temp, max_tokens)
                
#                 # Store both futures with their respective indices
#                 future_to_battle[future_1] = (index, i, j, 'first')
#                 future_to_battle[future_2] = (index, i, j, 'second')

#             # Process results as they complete
#             results_dict = {}  # Dictionary to hold results temporarily
#             for future in concurrent.futures.as_completed(future_to_battle):
#                 index, i, j, which = future_to_battle[future]
#                 try:
#                     result = future.result()
#                     if which == 'first':
#                         if (index, i, j) in results_dict:
#                             results_dict[(index, i, j)][0] = result
#                         else:
#                             results_dict[(index, i, j)] = [result, None]
#                     else:
#                         if (index, i, j) in results_dict:
#                             results_dict[(index, i, j)][1] = result  # Store result_2
#                         else:
#                             results_dict[(index, i, j)] = [None, result]
#                 except Exception as exc:
#                     print(f'Game {i} vs {j} ({which}) generated an exception: {exc}')
#                     results_dict[(index, i, j)] = [None, None]  # In case of failure, store None

#             # Convert dictionary to list to maintain order
#             results_list = [results_dict[(index, i, j)] for index, (i, j) in enumerate(round_robin_list)]

#         # Process results in order of the original round robin list
#         for (i, j), (result_1, result_2) in zip(round_robin_list, results_list):
#             # Use the same object that was chosen for the battle
#             object = result_1[1]  # Object is the same for both prompts
            
#             # Get initial ELO ratings
#             initial_elo_rating_1 = specific_elo_ratings_dict[top_prompts[i]]
#             initial_elo_rating_2 = specific_elo_ratings_dict[top_prompts[j]]

#             # Update ELO ratings using update_elo function
#             specific_elo_ratings_dict, winner = update_elo(specific_elo_ratings_dict, top_prompts[i], top_prompts[j], result_1, result_2)
            
#             # Get final ELO ratings
#             final_elo_rating1 = specific_elo_ratings_dict[top_prompts[i]]
#             final_elo_rating2 = specific_elo_ratings_dict[top_prompts[j]]

#             # Increment battle number and prepare battle dictionary
#             battle_number += 1
#             battle_dict = {
#                 'object_name': object,
#                 'prompt_1': {
#                     'questioner_prompt': result_1[0] if result_1 else top_prompts[i],
#                     'object': result_1[1] if result_1 else object,
#                     'questions_asked': result_1[2] if result_1 else [],
#                     'responses': result_1[3] if result_1 else [],
#                     'guesses': result_1[4] if result_1 else [],
#                     'no_of_questions': result_1[5] if result_1 else 0,
#                     'guessed_correctly': result_1[6] if result_1 else False,
#                     'initial_elo': initial_elo_rating_1,
#                     'final_elo': final_elo_rating1
#                 },
#                 'prompt_2': {
#                     'questioner_prompt': result_2[0] if result_2 else top_prompts[j],
#                     'object': result_2[1] if result_2 else object,
#                     'questions_asked': result_2[2] if result_2 else [],
#                     'responses': result_2[3] if result_2 else [],
#                     'guesses': result_2[4] if result_2 else [],
#                     'no_of_questions': result_2[5] if result_2 else 0,
#                     'guessed_correctly': result_2[6] if result_2 else False,
#                     'initial_elo': initial_elo_rating_2,
#                     'final_elo': final_elo_rating2
#                 },
#                 'winner': winner
#             }

#             # Add the battle dictionary to the league dictionary
#             league_dictionary[league_number][battle_number] = battle_dict

#         # Store the league_dictionary in the "final_league_results"+"z".json file
#         with open(f'final_league_results{z}.json', 'w') as json_file:
#             json.dump(league_dictionary, json_file, indent=4)
        

# # Function to just change the elo ratings of the prompts and store them in the input dictionary
# def update_elo(dict, questioner_prompt1, questioner_prompt2, result_1, result_2):

#     """
#     Parameters
#     ----------
#     dict: dict
#         The dictionary where the elo ratings of the prompts are to be stored
#     pro_prompt: str
#         The prompt for the Pro side debater
#     con_prompt: str
#         The prompt for the Con side debater
#     winner: str
#         The winner of the Debate Battle
#     """


 
#     no_of_questions1 = result_1[5]
#     guessed_correctly_1 = result_1[6]
#     no_of_questions2 = result_2[5]
#     guessed_correctly_2 = result_2[6]
#     # See who is the winner by comparing the number of questions asked as well as the guessed correctly, just set the winner and loser and if it is a draw then set the winner = "draw"
#     if no_of_questions1 < no_of_questions2:
#         winner = questioner_prompt1
#         loser = questioner_prompt2
#     elif no_of_questions1 > no_of_questions2:
#         winner = questioner_prompt2
#         loser = questioner_prompt1
#     else:
#         if guessed_correctly_1 > guessed_correctly_2:
#             winner = questioner_prompt1
#             loser = questioner_prompt2
#         elif guessed_correctly_1 < guessed_correctly_2:
#             winner = questioner_prompt2
#             loser = questioner_prompt1
#         else:
#             winner = "draw"
#             loser = "draw"

#     #If it is a draw, then calculate the ELO ratings of the prompts and print the updated ELO ratings
#     if winner == "draw":
#         # Find the ELO scores of the winner and loser
#         winner_rating = dict[questioner_prompt1]
#         loser_rating = dict[questioner_prompt2]
#         # Calculate the expected score of the winner and loser
#         expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
#         expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
#         # Update the ELO ratings of the winner and loser
#         winner_rating = winner_rating + 32 * (0.5 - expected_winner)
#         loser_rating = loser_rating + 32 * (0.5 - expected_loser)
#         # Update the ELO ratings in the dictionary
#         dict[questioner_prompt1] = winner_rating
#         dict[questioner_prompt2] = loser_rating
#         # Print the updated ELO ratings
#         print("Updated ELO Ratings:")
#         print(f"{questioner_prompt1}: {winner_rating}")
#         print(f"{questioner_prompt2}: {loser_rating}")

#     # Update the ELO ratings of the prompts if it is not a draw
#     elif winner != "draw":
#         # Find the ELO scores of the winner and loser
#         winner_rating = dict[winner]
#         loser_rating = dict[loser]
#         # Calculate the expected score of the winner and loser
#         expected_winner = 1 / (1 + 10 ** ((loser_rating - winner_rating) / 400))
#         expected_loser = 1 / (1 + 10 ** ((winner_rating - loser_rating) / 400))
#         # Update the ELO ratings of the winner and loser
#         winner_rating = winner_rating + 32 * (1 - expected_winner)
#         loser_rating = loser_rating + 32 * (0 - expected_loser)
#         # Update the ELO ratings in the dictionary
#         dict[winner] = winner_rating
#         dict[loser] = loser_rating
    
    
#     return dict, winner


# # Function to print a visually appealing output of the elo_rating function
# def print_elo_ratings(questioner_prompt1, questioner_prompt2, object):
#     """
#     Parameters
#     ----------
#     questioner_prompt1: str
#         The prompt given to the questioner to play the 20 questions game (First prompt)
#     questioner_prompt2: str
#         The prompt given to the questioner to play the 20 questions game (Second prompt)
#     object: str
#         The object which the answerer is thinking of
#     """
    
#     elo_ratings_dict[questioner_prompt1] = 1200
#     elo_ratings_dict[questioner_prompt2] = 1200

#     result_1, result_2, winner = elo_rating(questioner_prompt1, questioner_prompt2, object)
    
#     def print_game_details(result, game_number):
#         print(f"\nGame {game_number} Details:\n")
#         print(f"Questioner Prompt: {result[0]}\n")
#         print(f"Object: {result[1]}\n")
        
#         questions_asked = result[2]
#         responses = result[3]
#         guesses = result[4]
        
#         for i in range(len(questions_asked)):
#             print(f"Question {i + 1}: {questions_asked[i]}\n")
#             print(f"Response {i + 1}: {responses[i]}\n")
#             print(f"Guess {i + 1}: {guesses[i]}\n")
#             print("---\n")
        
#         if result[6]:
#             print("The questioner guessed the object correctly!\n")
#         else:
#             print("The questioner failed to guess the object within 20 questions.\n")
#         print(f"Number of Questions Asked: {result[5]}\n")
#         print("\n")
    
#     print_game_details(result_1, 1)
#     print_game_details(result_2, 2)
    
#     if winner == "draw":
#         print("The game is a draw!\n")
#     else:
#         print(f"The winner is: {winner}\n")


def main():

    global elo_ratings_dict
    global league_dict
    global top_bottom_prompts_dict
    global human_prompts_used_dict
    global current_league
    global top_bottom_prompts_dict_across_league 
    global origin_league_prompts_dict
    global top_bottom_prompts_dict_poc

    no_of_objects = 3
    current_directory = os.getcwd()
    temp = [0.5]
    max_tokens =[500]
    no_of_runs =0

    for no_of_prompts_start in range (10,21,2):
        for no_of_prompts_between in range (10,21,2):
            for k in range (5,6,1):
                for temperature in temp:
                    for max_token in max_tokens:
                        if(no_of_runs == 1):
                            break
                        print("no_of_prompts_start: ", no_of_prompts_start)
                        print("no_of_prompts_between: ", no_of_prompts_between)
                        print("k: ", k)
                        print("temperature: ", temperature)
                        print("max_token: ", max_token)
                        folder_name = "no_of_prompts_start_"+str(no_of_prompts_start)+"_no_of_prompts_between_"+str(no_of_prompts_between)+"_k_"+str(k)+"_temperature_"+str(temperature)+"_max_token_"+str(max_token)+"_no_of_objects_"+str(no_of_objects)+"_LLM_start"
                        new_directory = os.path.join(current_directory, folder_name)
                        # Check if the directory is already present then don't make the directory again
                        if os.path.exists(new_directory):
                            print(f"Directory {new_directory} already exists!")
                            
                        else:
                            os.makedirs(new_directory, exist_ok=True)

                        os.chdir(new_directory)
                        tournament(no_of_objects, no_of_prompts_start, no_of_prompts_between, k, 15, temperature, max_token, current_directory)
                        # Dictionary to store the ELO ratings of the prompts
                        elo_ratings_dict = {}
                        # Dictionary to store the history of the games in the leagues
                        league_dict = {}
                        #league number
                        current_league = 1
                        # Dictionary to store the top and bottom k prompts from each league
                        top_bottom_prompts_dict = {}
                        # Dictionary to store the human prompts which have been used in a league already
                        human_prompts_used_dict = {}
                        top_bottom_prompts_dict_across_league = {}
                        origin_league_prompts_dict = {}
                        top_bottom_prompts_dict_poc = {}
                        no_of_runs+=1

    
main()